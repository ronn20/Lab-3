{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0720443-3348-4e72-8d90-93c56fd25545",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b34fe-78eb-4842-b15c-c3c021e99faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chi2\n",
    "from IPython.display import clear_output \n",
    "from IPython.display import Image\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "import glob\n",
    "import math\n",
    "import mplcursors\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e12b4-2585-4053-adae-3598da40b735",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b114eed-0890-4bfd-8b68-1e8f12534603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fits_data_to_3d_array(file_list): \n",
    "    '''\n",
    "    Takes in a list of fits files and converts data into a 3d array\n",
    "    Parameters:\n",
    "        file_list (list): List containing paths of FITS files to convert.\n",
    "    Returns:\n",
    "        final_array: 3d array of data from each fits file.\n",
    "    '''    \n",
    "    for i, file in enumerate(file_list):\n",
    "        with fits.open(file) as hdu:\n",
    "            if i==0:  #If first iteration, create array with correct shape of data\n",
    "                final_array = np.zeros((len(file_list), *hdu[0].data.shape))\n",
    "            final_array[i,:,:] = hdu[0].data\n",
    "    return final_array\n",
    "\n",
    "#Creates a median combine of a 3d array\n",
    "def median_combine(frame_array_3d):\n",
    "    master_frame = np.median(frame_array_3d,axis=0) #takes median along the 'file_index' axis\n",
    "    return master_frame\n",
    " \n",
    "def save_array_to_fits_file(array, new_file_name):\n",
    "    '''\n",
    "    Saves a 2d array of pixel data to a FITS file.\n",
    "    Parameters:\n",
    "    array(2d): 2d array containing pixel values.\n",
    "    new_file_name (path): location, name of FITS file to be saved.\n",
    "    '''  \n",
    "    hdu = fits.PrimaryHDU(data = array)\n",
    "    hdul = fits.HDUList([hdu])\n",
    "    hdul.writeto(new_file_name,overwrite=True)\n",
    "\n",
    "\n",
    "def glob_files(folder, specifier, asterisks='both', show=False):\n",
    "    asterisk = \"*\"\n",
    "    if asterisks == 'both':\n",
    "        files = glob.glob(os.path.join(folder, asterisk + specifier + asterisk))\n",
    "    elif asterisks == 'left':\n",
    "        files = glob.glob(os.path.join(folder, asterisk + specifier))\n",
    "    elif asterisks == 'right':\n",
    "        files = glob.glob(os.path.join(folder, specifier + asterisk))\n",
    "    if show:\n",
    "        print(f'Number of Files: {len(files)}')\n",
    "        for file in files: \n",
    "            print(file)\n",
    "    return files\n",
    "\n",
    "def cut_pixel_data_array(pixel_data_array, y_pixel_min, y_pixel_max, print_shape=False):\n",
    "    '''\n",
    "    Returns an array of pixel data, cut to a specified vertical (y-pixel) range.\n",
    "    Parameters:\n",
    "        pixel_data_array (2d array): Array of pixel data to create a cut from.\n",
    "        y_pixel_min (int): Lower bound to start cut. \n",
    "        y_pixel_max (int): Upper bound to end cut. \n",
    "    Returns:\n",
    "        final_array: 3d array of data from each fits file.\n",
    "    '''   \n",
    "    lower_index = (-1) + y_pixel_min\n",
    "    upper_index = (-1) + y_pixel_max\n",
    "    pixel_data_array_cut = pixel_data_array[lower_index:upper_index,:]\n",
    "    if print_shape: print(pixel_data_array_cut.shape)\n",
    "    return pixel_data_array_cut\n",
    "\n",
    "def display_2d_array(pixel_data_array, figsize=(7,3), lower_percentile=1, upper_percentile=99,title='2D Array', cmap='gray', overlay_x=[], overlay_y=[]):\n",
    "    '''\n",
    "    Displays a 2d array of pixel data. Useful to see the image contained in a FITS file,\n",
    "    or to verify successful array data handling.\n",
    "    Parameters:\n",
    "        pixel_data_array (2d array): Array of pixel data to display.\n",
    "        lower_percentile (int): Adjusts the pixel scale lower bound; default = 1\n",
    "        upper_percentile (int): Adjusts the pixel scale upper bound; default = 99\n",
    "    '''   \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    minimum = np.percentile(pixel_data_array,lower_percentile)\n",
    "    maximum = np.percentile(pixel_data_array,upper_percentile)\n",
    "    img = ax.imshow(pixel_data_array, cmap=cmap, origin = 'lower', vmin=minimum, vmax=maximum)\n",
    "    cbar = plt.colorbar(img, ax=ax, orientation='horizontal', pad=0.1)\n",
    "    cbar.set_label('Pixel Counts/Intensity')\n",
    "    if overlay_x != []:\n",
    "        plt.scatter(overlay_x,overlay_y,color='r', s=5)\n",
    "    plt.title(title)\n",
    "\n",
    "def darks_correction(mcombine_array, darks_mcombine_array):\n",
    "    return mcombine_array - darks_mcombine_array\n",
    "\n",
    "def darks_correction_uncertianty(mcombine_unc_array, darks_mcombine_unc_array):\n",
    "    '''\n",
    "    Can also be used for multiplication/division of two quantities w uncertainties  \n",
    "    '''\n",
    "    return np.sqrt(mcombine_unc_array**2 + darks_mcombine_unc_array**2)\n",
    "\n",
    "def flat_correction_uncertainty(sci,sci_uncert,flat,flat_uncert):\n",
    "    radical = ((sci*flat_uncert) / flat**2)**2 + (sci_uncert/flat)**2\n",
    "    return np.sqrt(radical)\n",
    "\n",
    "def quadrature_unc(uncertainties_array, axis=0):\n",
    "    return np.sqrt(np.nansum(np.square(uncertainties_array),axis=axis))\n",
    "\n",
    "def calibrate_fits_files(fits_file_list=[], output_3d_only=False, output_mcombine_only=True, output_darks_corrected=True,\n",
    "                         mcombine_array=None, darks_array=None, display_array=False):\n",
    "    '''\n",
    "    Function is multifunctional. It can:\n",
    "    1) Output a 3d array\n",
    "    2) Output a median combine (2d)\n",
    "    3) Output a darks corrected array (2d)\n",
    "\n",
    "    fits_file_list([]) : List containing paths of FITS files to convert.\n",
    "    output_3d_only(True/False) : Output only 3d array\n",
    "    output_mcombine_only(True/False) : Output only mcombine array\n",
    "    output_darks_corrected(True/False) : Output only darks corrected array\n",
    "    mcombine_array : \n",
    "    darks_array :\n",
    "    display_array(True/False) :\n",
    "    '''\n",
    "    if output_3d_only:\n",
    "        array_3d = fits_data_to_3d_array(fits_file_list)\n",
    "        return array_3d\n",
    "    elif output_mcombine_only:\n",
    "        array_3d = fits_data_to_3d_array(fits_file_list)\n",
    "        array_mcombine = median_combine(array_3d)\n",
    "        array_std = np.std(array_3d, axis=0) / np.sqrt(np.shape(array_3d)[0])\n",
    "        if display_array:\n",
    "            display_2d_array(array_mcombine)\n",
    "        return array_mcombine, array_std\n",
    "    elif output_darks_corrected:\n",
    "        final_array = darks_correction(mcombine_array - darks_array)\n",
    "        if display_array:\n",
    "            display_2d_array(final_array)\n",
    "        return final_array\n",
    "\n",
    "def mad(data, axis=0):\n",
    "    \"\"\"\n",
    "    Calculate the Median Absolute Deviation (MAD) of the given data along the specified axis.\n",
    "    Parameters:\n",
    "    - data: 2D array of data\n",
    "    - axis: Axis along which to compute the MAD (default is 0 for columns)\n",
    "    Returns:\n",
    "    - mad_values: MAD for each column (1D array)\n",
    "    \"\"\"\n",
    "    medians = np.median(data, axis=axis)\n",
    "    #absolute deviations from the median\n",
    "    abs_devs = np.abs(data - medians[np.newaxis, :])\n",
    "    #median of the absolute deviations\n",
    "    mad_values = np.median(abs_devs, axis=axis)\n",
    "    return mad_values\n",
    "\n",
    "def median_uncertainty(data, axis=0):\n",
    "    \"\"\"\n",
    "    Calculate the uncertainty of the median for each column in the data, using the MAD.\n",
    "    Parameters:\n",
    "    - data: 2D array of data\n",
    "    - axis: Axis along which to compute the median uncertainty (default is 0 for columns)\n",
    "    Returns:\n",
    "    - uncertainty: Uncertainty of the median for each column (1D array)\n",
    "    \"\"\"\n",
    "    mad_values = mad(data, axis=axis)\n",
    "    uncertainty = 1.4826 * mad_values / np.sqrt(data.shape[0])\n",
    "    return uncertainty\n",
    "\n",
    "def normalize_to_unity(array_2d, ctype='sum', axis=0, array_uncert=[], return_uncert=False):\n",
    "    if ctype == 'sum':\n",
    "        factor = np.max(np.nansum(array_2d, axis=axis))\n",
    "        if return_uncert:\n",
    "            return quadrature_unc(array_uncert, axis=0) / factor\n",
    "        else: \n",
    "            return np.nansum(array_2d, axis=axis) / factor \n",
    "    elif ctype == 'mean':\n",
    "        factor = np.max(np.mean(array_2d, axis=axis))\n",
    "        if return_uncert:\n",
    "            sigma = np.std(array_uncert, axis=0) / np.sqrt(array_uncert.shape[0])\n",
    "            return  sigma / factor\n",
    "        else: \n",
    "            return np.mean(array_2d, axis=axis) / factor \n",
    "    elif ctype == 'median':\n",
    "        factor = np.max(np.median(array_2d, axis=axis))\n",
    "        if return_uncert: \n",
    "            return median_uncertainty(array_uncert) / factor\n",
    "        else: \n",
    "            return np.median(array_2d, axis=axis) / factor     \n",
    "\n",
    "def normalize_to_unity_1d(array_1d, array_uncert_1d):\n",
    "    '''\n",
    "    Returns: \n",
    "    - normalized_array, normalized_uncert_array\n",
    "    '''\n",
    "    factor = np.nanmax(array_1d)\n",
    "    normalized_array = array_1d / factor\n",
    "    normalized_uncert_array = array_uncert_1d / factor\n",
    "    return normalized_array, normalized_uncert_array\n",
    "\n",
    "def display_cut_spectrum(pixel_data_array, figsize=(12,5), lower_percentile=1, upper_percentile=99, title='2D Array', cmap='gist_gray'):\n",
    "    '''\n",
    "    Displays a 2d array of pixel data. Useful to see the image contained in a FITS file,\n",
    "    or to verify successful array data handling.\n",
    "    Parameters:\n",
    "        pixel_data_array (2d array): Array of pixel data to display.\n",
    "        lower_percentile (int): Adjusts the pixel scale lower bound; default = 1\n",
    "        upper_percentile (int): Adjusts the pixel scale upper bound; default = 99\n",
    "    '''   \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    minimum = np.percentile(pixel_data_array,lower_percentile)\n",
    "    maximum = np.percentile(pixel_data_array,upper_percentile)\n",
    "    img = ax.imshow(pixel_data_array, cmap=cmap, origin = 'lower', vmin=minimum, vmax=maximum)\n",
    "    #seismic RdGy gist_gray\n",
    "    cbar = plt.colorbar(img, ax=ax, orientation='horizontal', pad=0.1, fraction=.05, aspect=50)\n",
    "    cbar.set_label('Pixel Counts/Intensity')\n",
    "    fig.tight_layout\n",
    "    plt.title(title)\n",
    "    plt.plot()\n",
    "\n",
    "def cut_pixel_data_array_3d(pixel_data_array, y_pixel_min, y_pixel_max):\n",
    "    '''\n",
    "    Returns an array of pixel data, cut to a specified vertical (y-pixel) range.\n",
    "    Parameters:\n",
    "        pixel_data_array (2d array): Array of pixel data to create a cut from.\n",
    "        y_pixel_min (int): Lower bound to start cut. \n",
    "        y_pixel_max (int): Upper bound to end cut. \n",
    "    Returns:\n",
    "        final_array: 3d array of data from each fits file.\n",
    "    '''   \n",
    "    lower_index = (-1) + y_pixel_min\n",
    "    upper_index = (-1) + y_pixel_max\n",
    "    pixel_data_array_cut = pixel_data_array[:, lower_index:upper_index, :]\n",
    "    return pixel_data_array_cut\n",
    "\n",
    "def plot_spectrum(spectrum_label, index_set, invert_spectrum=False, find_peaks=False, plot_Habg_balmer=[False, False, False], \n",
    "                  xlim=[], ylim=[], legend_loc='upper right', peaks_height=(0, 0.35), dy=0):\n",
    "    '''\n",
    "    spectrum_label : Ne_set1 Ne_set2 Ha_set1 Ha_set2 Ar_set1 Ar_set2 Hbg_set1 Hbg_set2\n",
    "    index_set : Ne_x_index_set1 Ne_x_index_set2 Ar_x_index_set1 Ar_x_index_set2\n",
    "    '''\n",
    "    if invert_spectrum:\n",
    "        c = -1.0\n",
    "        d = 1.0\n",
    "    else:\n",
    "        c = 1.0\n",
    "        d = 0.0\n",
    "    spectrum_index = np.arange(len(normalized_spectra[spectrum_label]))  # pixel index\n",
    "    spectrum_wavelength_index = pixel_to_wavelength_index(spectrum_index, *calibration_stats[index_set])  # pixel index --> wavelength index\n",
    "    \n",
    "    if find_peaks:\n",
    "        # Find Peaks\n",
    "        peaks, _ = find_peaks(normalized_spectra[spectrum_label], height=peaks_height)\n",
    "        # Annotate peaks\n",
    "        for peak in peaks:\n",
    "            plt.annotate(f'{spectrum_wavelength_index[peak]:.1f} nm', \n",
    "                         (spectrum_wavelength_index[peak], normalized_spectra[spectrum_label][peak]), \n",
    "                         xytext=(0, 4), textcoords='offset points', ha='center', fontsize=7, color='red')\n",
    "\n",
    "    plt.plot(spectrum_wavelength_index, c*normalized_spectra[spectrum_label]+d+dy, label=spectrum_label + ' Calibrated Spectrum')\n",
    "    plt.ylabel('Intensity')\n",
    "    plt.xlabel('Wavelength (nm)')\n",
    "    plt.yticks(np.arange(0.0, 1.1, 0.1))\n",
    "\n",
    "    if xlim != []: \n",
    "        plt.xlim(xlim)\n",
    "    if ylim != []: \n",
    "        plt.ylim(ylim)\n",
    "\n",
    "    # Plot Balmer Lines:\n",
    "    if plot_Habg_balmer[0]:\n",
    "        plt.vlines(x=656.3, ymin=0, ymax=1, color='r', label=\"Hα 656.3 nm\")\n",
    "    if plot_Habg_balmer[1]:\n",
    "        plt.vlines(x=486.1, ymin=0, ymax=1, color='cyan', label=\"Hβ 486.1 nm\")\n",
    "    if plot_Habg_balmer[2]:\n",
    "        plt.vlines(x=434, ymin=0, ymax=1, color='blue', label=\"Hγ 434 nm\")\n",
    "\n",
    "    plt.legend(loc=legend_loc)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039458f",
   "metadata": {},
   "source": [
    "## Creating Calibrated Science Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb1f85-9441-48a1-9244-5dd17114e756",
   "metadata": {},
   "source": [
    "### Creating Median Combine Dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6266d9ea-c233-4443-8d80-c066ad794573",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Darks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693e729a-6db6-4c8a-9a20-0057ebbbf55e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_files = True\n",
    "show_arrays = True\n",
    "darks_exposure_times = ['dark_3s', 'dark_10s', 'dark_15s', 'dark_25s', 'dark_60s']\n",
    "darks_mcombines = {}\n",
    "darks_mcombines_unc = {}\n",
    "\n",
    "for exposure in darks_exposure_times:\n",
    "    fits_files = glob_files('night_2', exposure, show=show_files)\n",
    "    darks_mcombines[exposure], darks_mcombines_unc[exposure] = calibrate_fits_files(fits_file_list=fits_files, display_array=show_arrays)    \n",
    "    #display_2d_array(darks_mcombines_unc[exposure])\n",
    "    #plt.title(f'{exposure} uncertainty')\n",
    "    #print(exposure, np.max(darks_mcombines_unc[exposure]))\n",
    "    \n",
    "for i in darks_mcombines:\n",
    "    print(f\"Avg pixel value in {i}: {np.mean(darks_mcombines[i])}\")\n",
    "    \n",
    "print('Darks Median Combines')\n",
    "for key, value in darks_mcombines.items():\n",
    "    print(f\"{key}, {np.shape(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75c517-7e35-4a5b-972c-b56c8e97d6a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Calibration Spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a343b-f3f5-4403-b67a-c156ace9de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_files = True\n",
    "show_arrays = True\n",
    "spectra = ['ha_neon_calibration_spectrum_set1', 'ha_neon_calibration_spectrum_set2_15s', 'hbg_argon_calibration_spectrum_sets1and2']\n",
    "spectra_calib = {}\n",
    "spectra_calib_unc = {}\n",
    "\n",
    "for spectrum in spectra:\n",
    "    fits_files = glob_files('night_2', spectrum, show=show_files)\n",
    "    spectra_calib[spectrum], spectra_calib_unc[spectrum] = calibrate_fits_files(fits_file_list=fits_files, display_array=show_arrays)\n",
    "    #display_2d_array(spectra_calib_unc[spectrum])\n",
    "new_keys = {\n",
    "    'ha_neon_calibration_spectrum_set1': 'neon_set1_15s',\n",
    "    'ha_neon_calibration_spectrum_set2_15s': 'neon_set2_15s',\n",
    "    'hbg_argon_calibration_spectrum_sets1and2': 'argon_10s'\n",
    "}\n",
    "for dictionary in [spectra_calib, spectra_calib_unc]:\n",
    "    updated_dict = {new_keys.get(old_key, old_key): value for old_key, value in dictionary.items()}\n",
    "    dictionary.clear()\n",
    "    dictionary.update(updated_dict)\n",
    "\n",
    "print('Spectra Median Combines')\n",
    "for key, value in spectra_calib.items():\n",
    "    print(f\"{key}, {np.shape(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648058b-86b7-4755-b1c2-460ef9b4e4ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Flats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f662d-7610-469f-91f5-ff3edfb1512f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_files = True\n",
    "show_arrays = True\n",
    "flats_names = ['Ha_Neon_flats_3s', 'hbg_flat_spectrum_sets1and2']\n",
    "flats_mcombines = {}\n",
    "flats_mcombines_unc = {}\n",
    "\n",
    "for name in flats_names:\n",
    "    fits_files = glob_files('night_2', name, show=show_files)\n",
    "    flats_mcombines[name], flats_mcombines_unc[name] = calibrate_fits_files(fits_file_list=fits_files, display_array=show_arrays)\n",
    "    #display_2d_array(flats_mcombines_unc[name])\n",
    "    \n",
    "new_keys = {\n",
    "    'Ha_Neon_flats_3s': 'HaNe_flats_3s',\n",
    "    'hbg_flat_spectrum_sets1and2': 'HbgAr_flats_60s',\n",
    "}\n",
    "for dictionary in [flats_mcombines, flats_mcombines_unc]:\n",
    "    updated_dict = {new_keys.get(old_key, old_key): value for old_key, value in dictionary.items()}\n",
    "    dictionary.clear()\n",
    "    dictionary.update(updated_dict)\n",
    "\n",
    "print('Flats Median Combines')\n",
    "for key, value in flats_mcombines.items():\n",
    "    print(f\"{key}, {np.shape(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5bb814-c448-4992-a84f-e667e348310a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Science Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47de4eb-4f59-4609-aed0-13ce1aa42238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_files = True\n",
    "show_arrays = True\n",
    "names = ['ha_lights_set1', 'ha_lights_set2', 'hbg_light_set1', 'hbg_light_set2']\n",
    "sci_mcombines = {}\n",
    "sci_mcombines_unc = {}\n",
    "for name in names:\n",
    "    fits_files = glob_files('night_2', name, show=show_files)\n",
    "    sci_mcombines[name], sci_mcombines_unc[name] = calibrate_fits_files(fits_file_list=fits_files, display_array=show_arrays) #\n",
    "    #display_2d_array(sci_mcombines_unc[name])\n",
    "    \n",
    "new_keys = {\n",
    "    'ha_lights_set1': 'Ha_sci_set1',\n",
    "    'ha_lights_set2': 'Ha_sci_set2',\n",
    "    'hbg_light_set1': 'Hbg_sci_set1',\n",
    "    'hbg_light_set2': 'Hbg_sci_set2',\n",
    "}\n",
    "for dictionary in [sci_mcombines, sci_mcombines_unc]:\n",
    "    updated_dict = {new_keys.get(old_key, old_key): value for old_key, value in dictionary.items()}\n",
    "    dictionary.clear()\n",
    "    dictionary.update(updated_dict)\n",
    "    \n",
    "print('Science Median Combines')\n",
    "for key, value in sci_mcombines.items():\n",
    "    print(f\"{key}, {np.shape(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066499bf-4dfb-4918-95b8-616b985e1996",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c1f46-ff78-429d-b30e-14da59371f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrate_fits_files(fits_file_list=glob_files('night_2','ha_laser_sanity'), display_array=True)\n",
    "calibrate_fits_files(fits_file_list=glob_files('night_2','hbg_Hlamp_spectrum_sanity'), display_array=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66c7c3-c3f2-4300-81f6-a5c51f815f94",
   "metadata": {},
   "source": [
    "Dicts: \\\n",
    "darks_mcombines \\\n",
    "spectra_calib \\\n",
    "flats_mcombines \\\n",
    "sci_mcombines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57401628-4e54-47a0-96a0-9fa5582c1d9a",
   "metadata": {},
   "source": [
    "### Dark Correction\n",
    "These will all be stored in dictionaries named:  \n",
    "- *`name_dcorr`*  \n",
    "- *`name_dcorr_unc`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e40ab8-0224-46f8-8bec-f9574e9a45cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spectra_dcorr = spectra_calib.copy()\n",
    "flats_dcorr = flats_mcombines.copy()\n",
    "sci_dcorr = sci_mcombines.copy()\n",
    "spectra_dcorr_unc = spectra_calib.copy()\n",
    "flats_dcorr_unc = flats_mcombines.copy()\n",
    "sci_dcorr_unc = sci_mcombines.copy()\n",
    "\n",
    "os.makedirs('calibrated_fits', exist_ok=True)\n",
    "\n",
    "#Science:\n",
    "for (key, value), (key_u, value_u) in zip(sci_mcombines.items(), sci_mcombines_unc.items()):\n",
    "    sci_dcorr[key] = darks_correction(value, darks_mcombines['dark_25s'])\n",
    "    sci_dcorr_unc[key_u] = darks_correction_uncertianty(value_u, darks_mcombines_unc['dark_25s'])\n",
    "    display_2d_array(sci_dcorr[key], title=key)\n",
    "    #display_2d_array(sci_dcorr[key] + sci_dcorr_unc[key], title=key)\n",
    "    save_array_to_fits_file(sci_dcorr[key], path.join('calibrated_fits', 'sci_' + key + 'dark_correction.FIT'))\n",
    "\n",
    "#Spectra:\n",
    "darks_order = ['dark_15s','dark_15s','dark_10s']\n",
    "for (key, value), dark_key in zip(spectra_calib.items(), darks_order):\n",
    "    spectra_dcorr[key] = darks_correction(value, darks_mcombines[dark_key])\n",
    "    spectra_dcorr_unc[key] = darks_correction_uncertianty(spectra_calib_unc[key], darks_mcombines_unc[dark_key])\n",
    "    display_2d_array(spectra_dcorr[key], title=key)\n",
    "    #display_2d_array(spectra_dcorr[key] + spectra_dcorr_unc[key], title=key)\n",
    "    save_array_to_fits_file(spectra_dcorr[key], path.join('calibrated_fits', 'spec_' +  key + 'dark_correction.FIT'))\n",
    "\n",
    "#Flats\n",
    "darks_order = ['dark_3s','dark_60s']\n",
    "for (key, value), dark_key in zip(flats_mcombines.items(), darks_order):\n",
    "    flats_dcorr[key] = darks_correction(value, darks_mcombines[dark_key])\n",
    "    flats_dcorr_unc[key] = darks_correction_uncertianty(flats_mcombines_unc[key], darks_mcombines_unc[dark_key])\n",
    "    display_2d_array(flats_dcorr[key], title=key)\n",
    "    #display_2d_array(flats_dcorr[key] + flats_dcorr_unc[key], title=key)\n",
    "    save_array_to_fits_file(flats_dcorr[key], path.join('calibrated_fits', 'flat_' +  key + 'dark_correction.FIT'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0893f561-cc14-4e1b-9099-8b29fa1fde96",
   "metadata": {},
   "source": [
    "### Defining Cuts for 25 um slit\n",
    "We need to cut these arrays to only look at the slit that our star is in ($25 \\mu m$). We will worry about obtaining the target signal from the science frames in the Gaussian Cutting section.\\\n",
    "Looking at the flats in ds9, here are the cut ranges for our frames: \\\n",
    "\\\n",
    "H$\\alpha$ and Neon:  $107$ $\\le$ $y_{pix}$ $\\le$ $171$ \\\n",
    "H$\\beta$, H$\\gamma$ (Set 1):  $114$ $\\le$ $y_{pix}$ $\\le$ $177$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677bd276-63aa-4f45-b001-9ba2bd480444",
   "metadata": {},
   "source": [
    "Lets cut these science arrays first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0970103-6bfe-4b5d-98fa-e7e0965e0bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a dict of cuts\n",
    "cut1 = [107,171]\n",
    "cut2 = [114,177]\n",
    "cuts_dict = {'Ha_1' : cut1,\n",
    "             'Ha_2' : cut1,\n",
    "             'Hgb_1' : cut2,\n",
    "             'Hgb_2' : cut2,\n",
    "             }\n",
    "#Create dicts to store cut arrays\n",
    "sci_cut = sci_dcorr.copy()\n",
    "sci_cut_unc = sci_dcorr_unc.copy()\n",
    "os.makedirs('cut_fits', exist_ok=True) #New directory\n",
    "\n",
    "#Perform cuts\n",
    "for (key, value), cut_key in zip(sci_cut.items(), cuts_dict):\n",
    "    sci_cut[key] = cut_pixel_data_array(value, *cuts_dict[cut_key])\n",
    "    sci_cut_unc[key] = cut_pixel_data_array(sci_cut_unc[key], *cuts_dict[cut_key])\n",
    "    display_cut_spectrum(sci_cut[key], title=key)\n",
    "    save_array_to_fits_file(sci_cut[key], path.join('cut_fits', 'sci_' +  key + '_cut.FIT'))\n",
    "\n",
    "for key, value in sci_cut.items():\n",
    "    print(f\"{key}, {np.shape(value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaf5067-6105-4460-bee2-8cfd13550cbf",
   "metadata": {},
   "source": [
    "Now cut the flats and calibration spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78601c-db43-4056-9452-8b12d53bff73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flats_cut = flats_dcorr.copy()\n",
    "flats_cut_unc = flats_dcorr_unc.copy()\n",
    "spectra_cut = spectra_dcorr.copy()\n",
    "spectra_cut_unc = spectra_dcorr_unc.copy()\n",
    "\n",
    "flats_cut = {'Ha_flat_set1' : cut_pixel_data_array(flats_dcorr['HaNe_flats_3s'], *cuts_dict['Ha_1']),\n",
    "             'Ha_flat_set2' : cut_pixel_data_array(flats_dcorr['HaNe_flats_3s'], *cuts_dict['Ha_2']),\n",
    "             'Hbg_flat_set1' : cut_pixel_data_array(flats_dcorr['HbgAr_flats_60s'], *cuts_dict['Hgb_1']),\n",
    "             'Hbg_flat_set2' : cut_pixel_data_array(flats_dcorr['HbgAr_flats_60s'], *cuts_dict['Hgb_2']),\n",
    "            }\n",
    "flats_cut_unc = {'Ha_flat_set1' : cut_pixel_data_array(flats_dcorr_unc['HaNe_flats_3s'], *cuts_dict['Ha_1']),\n",
    "                 'Ha_flat_set2' : cut_pixel_data_array(flats_dcorr_unc['HaNe_flats_3s'], *cuts_dict['Ha_2']),\n",
    "                 'Hbg_flat_set1' : cut_pixel_data_array(flats_dcorr_unc['HbgAr_flats_60s'], *cuts_dict['Hgb_1']),\n",
    "                 'Hbg_flat_set2' : cut_pixel_data_array(flats_dcorr_unc['HbgAr_flats_60s'], *cuts_dict['Hgb_2']),\n",
    "                }\n",
    "spectra_cut = {'Ne_spect_set1' : cut_pixel_data_array(spectra_dcorr['neon_set1_15s'], *cuts_dict['Ha_1']),\n",
    "               'Ne_spect_set2' : cut_pixel_data_array(spectra_dcorr['neon_set2_15s'], *cuts_dict['Ha_2']),\n",
    "               'Ar_spect_set1' : cut_pixel_data_array(spectra_dcorr['argon_10s'], *cuts_dict['Hgb_1']),\n",
    "               'Ar_spect_set2' : cut_pixel_data_array(spectra_dcorr['argon_10s'], *cuts_dict['Hgb_2']),\n",
    "              }\n",
    "spectra_cut_unc = {'Ne_spect_set1' : cut_pixel_data_array(spectra_dcorr_unc['neon_set1_15s'], *cuts_dict['Ha_1']),\n",
    "                   'Ne_spect_set2' : cut_pixel_data_array(spectra_dcorr_unc['neon_set2_15s'], *cuts_dict['Ha_2']),\n",
    "                   'Ar_spect_set1' : cut_pixel_data_array(spectra_dcorr_unc['argon_10s'], *cuts_dict['Hgb_1']),\n",
    "                   'Ar_spect_set2' : cut_pixel_data_array(spectra_dcorr_unc['argon_10s'], *cuts_dict['Hgb_2']),\n",
    "                  }\n",
    "\n",
    "for key,value in flats_cut.items():\n",
    "    display_cut_spectrum(flats_cut[key], title=key)\n",
    "    save_array_to_fits_file(flats_cut[key], path.join('cut_fits', 'flat_' +  key + '_cut.FIT'))\n",
    "    \n",
    "for key,value in spectra_cut.items():\n",
    "    display_cut_spectrum(spectra_cut[key], title=key) \n",
    "    save_array_to_fits_file(spectra_cut[key], path.join('cut_fits', 'spect_' +  key + '_cut.FIT'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529d9861-9171-4de3-84ad-633385ed3628",
   "metadata": {},
   "source": [
    "### Flat Correction\n",
    "We will try two different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db5794-714a-4d79-9075-36af79b7dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Polyfit = True\n",
    "FlatDivision = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8855d-77c5-4262-b8b3-2077bcfdf6b3",
   "metadata": {},
   "source": [
    "#### Method 1: Polynomial Fit\n",
    "Now it is time for the flat correction. Here are the steps:\n",
    "1) Take the 2D array of the flat field and find the median value of each column. This will create a 1D array with intensity values corresponding to horizontal pixel positions.\n",
    "2) Plot the 1D array vs pixel position.\n",
    "3) Fit a polynomial through the intensity curve.\n",
    "4) Divide all data by this curve fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb76e95-d2cc-48b4-ab00-7cc6a1db588e",
   "metadata": {},
   "source": [
    "##### 1 and 2) Creating 1D Flat Field Arrays and Plotting Along Dispersion Axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6530df-7783-43f3-b577-e6e0f3e54ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Polyfit:\n",
    "    flat_field_col_medians = {}\n",
    "    flat_field_col_medians_unc = {}\n",
    "    \n",
    "    for key,value in flats_cut.items():\n",
    "        flat_field_col_medians[key + '_col_median'] = np.median(value, axis=0)\n",
    "        flat_field_col_medians_unc[key + '_col_median'] = median_uncertainty(value, axis=0)\n",
    "        \n",
    "    indices = np.arange(len(flat_field_col_medians['Ha_flat_set1_col_median']))\n",
    "    \n",
    "    for key,value in flat_field_col_medians.items():\n",
    "        label = key.replace('_flat_',' ').replace('_col_median','')\n",
    "        plt.plot(indices, value, label=label, ls='solid',)# marker='.', ms=0)\n",
    "        #plt.errorbar(indices, value, yerr=flat_field_col_medians_unc[key], fmt='.', ms=1, lw=1,label=label + ' Uncertainty')\n",
    "        plt.title('Pixel Counts vs Position Along Dispersion Axis')\n",
    "        plt.xlabel('Dispersion Axis (Horizontal Pixels)')\n",
    "        plt.ylabel('Pixel Counts')\n",
    "        plt.legend()\n",
    "        plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b2632-1509-4dae-986c-9f773f01aa01",
   "metadata": {},
   "source": [
    "##### 3 and 4) Polynomial Fit and Flat Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83c54c-58d0-4036-85f9-5b16f69b2f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Polyfit:\n",
    "    specific_plot = False\n",
    "    specific_plot_label = 'Ha_flat_set1'\n",
    "    os.makedirs('normalized_fits', exist_ok=True)\n",
    "    normalized_flat_fields = {}\n",
    "    normalized_flat_fields_unc = {}\n",
    "    degree = 50\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(flat_field_col_medians))) #plasma viridis\n",
    "    \n",
    "    for i, ((med_key, med_values), (cut_flat_key, cut_flat_values)) in enumerate(zip(flat_field_col_medians.items(), flats_cut.items())): \n",
    "        if specific_plot:\n",
    "            if cut_flat_key != specific_plot_label:  # Change the condition to choose which set to display\n",
    "                continue\n",
    "        print(med_key,cut_flat_key)\n",
    "        coefficients = np.polyfit(indices, med_values, deg=degree)\n",
    "        polynomial_fit = np.polyval(coefficients, indices)\n",
    "        polynomial_fit_2d = np.tile(polynomial_fit, (cut_flat_values.shape[0], 1))\n",
    "        ax.plot(indices, polynomial_fit, color=colors[i], label=f'{med_key} Fit')\n",
    "        ax.scatter(indices, med_values, marker='o', s=3, color=colors[i], label=f'{med_key}')\n",
    "        ax.set_ylabel('Pixel Value')\n",
    "        ax.set_xlabel('Pixel Index')\n",
    "        ax.set_title('Polynomial Fit to Flat Field Data')\n",
    "        ax.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "        plt.legend()\n",
    "        \n",
    "        normalized_flat_fields[cut_flat_key] = flats_cut[cut_flat_key] / polynomial_fit_2d\n",
    "        normalized_flat_fields_unc[cut_flat_key] = flats_cut_unc[cut_flat_key] / polynomial_fit_2d   # good approximation if the fit is good      \n",
    "        save_array_to_fits_file(normalized_flat_fields[cut_flat_key], path.join('normalized_fits', 'norm_' +  cut_flat_key + '.FIT'))\n",
    "        \n",
    "    ax.legend(loc='upper right', frameon=False, fontsize=10, markerscale=2)\n",
    "    # Show the plot\n",
    "    plt.tight_layout()  # Adjusts layout to fit everything\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f17e9-5d1b-4fe7-b78d-ad67a3a3e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Polyfit:\n",
    "    for key, values in normalized_flat_fields.items():\n",
    "        display_cut_spectrum(values, title=key+'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91012e8-0189-48d2-95a4-01a93ca641b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Polyfit:\n",
    "    display_2d_array(flats_cut['Ha_flat_set1'])\n",
    "    plt.title('Non-Normalized Flat Field')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f5f715-6ea5-4d33-819d-fe83c4a0f880",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if Polyfit:\n",
    "    display_2d_array(normalized_flat_fields['Ha_flat_set1'])\n",
    "    plt.title('Normalized Flat Field')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c098b-63cd-4f36-8101-ee73881ac38e",
   "metadata": {},
   "source": [
    "Now we have to finish calibrating our spectra (both sci and calibration) by dividing by the flat field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635071c9-55f0-401b-ba87-e592398cd387",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def uncertainty_division(numerator, denominator, numerator_unc, denominator_unc):\n",
    "    #epsilon = 1e-10  # Small value to prevent division by zero\n",
    "    #safe_denominator = np.where(denominator == 0, epsilon, denominator)\n",
    "    var1 = np.square(numerator_unc / denominator)\n",
    "    var2 = np.square((numerator * denominator_unc) / np.square(denominator))\n",
    "    return np.sqrt(var1 + var2)\n",
    "\n",
    "if Polyfit: \n",
    "    calibrated_science = sci_cut.copy()\n",
    "    calibrated_science_unc = sci_cut_unc.copy()\n",
    "    calibrated_spectra = spectra_cut.copy()\n",
    "    calibrated_spectra_unc = spectra_cut_unc.copy()\n",
    "\n",
    "    os.makedirs('finalized_spectra', exist_ok=True)\n",
    "    for (key, value), (flat_key, flat_value) in zip(calibrated_science.items(), normalized_flat_fields.items()):\n",
    "        calibrated_science[key] = value / flat_value \n",
    "        calibrated_science_unc[key] = uncertainty_division(sci_cut[key], normalized_flat_fields[flat_key], sci_cut_unc[key], normalized_flat_fields_unc[flat_key])\n",
    "        display_cut_spectrum(value, title=key)\n",
    "        save_array_to_fits_file(value, path.join('finalized_spectra',f'calibrated_{key}.FIT'))\n",
    "    for (key, value), (flat_key, flat_value), (skey,svalue) in zip(calibrated_spectra.items(), normalized_flat_fields.items(), spectra_cut.items()):\n",
    "        calibrated_spectra[key] = value / flat_value   \n",
    "        calibrated_spectra_unc[key] = uncertainty_division(spectra_cut[skey], normalized_flat_fields[flat_key], spectra_cut_unc[skey], normalized_flat_fields_unc[flat_key])\n",
    "        display_cut_spectrum(value, title=key)\n",
    "        save_array_to_fits_file(value, path.join('finalized_spectra',f'calibrated_{key}.FIT'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90ffd30-03e4-4fc7-868b-f7955e736328",
   "metadata": {},
   "source": [
    "#### Method 2: Flat Field Division"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1780427b",
   "metadata": {},
   "source": [
    "##### Normalize The Flat Fields\n",
    "Divide the flat fields by the median of the flat field to normalize them, \\\n",
    "The intensity of the calibration spectrum does not matter as much to us so we are okay with proceeding cosi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829301e-1a71-42ac-b46d-da64a70ab679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FlatDivision:\n",
    "    value = flats_cut['Ha_flat_set1']\n",
    "    print(np.median(value),np.mean(value))\n",
    "    plt.hist(value.flatten(), bins=50, range=(0, np.max(value)), histtype='step', color='blue')\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b741b7-87e6-4efe-8e85-4589f04bfe62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FlatDivision:\n",
    "    normalized_flat_fields = flats_cut.copy()\n",
    "    normalized_flat_fields_unc = flats_cut_unc.copy()\n",
    "    for (key,value), (ukey,uvalue) in zip(normalized_flat_fields.items(), normalized_flat_fields_unc.items()):\n",
    "        print(np.median(value),np.mean(value))\n",
    "        v = value / np.median(value)\n",
    "        normalized_flat_fields[key] = v\n",
    "        print(np.median(value),np.mean(value))\n",
    "        normalized_flat_fields_unc[ukey] = uvalue / np.median(value)\n",
    "        #print(np.median(normalized_flats_unc[ukey]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e97cca-950e-4368-bb6b-0691cf984a1b",
   "metadata": {},
   "source": [
    "##### Apply Flat Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33295d76-dd06-45b4-86c5-347035d63a34",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Display normalized flat fields\n",
    "if FlatDivision:\n",
    "    for key, values in normalized_flat_fields.items():\n",
    "        display_cut_spectrum(values, title=key+'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bdd42e-380d-43be-975e-becf35eb8e2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FlatDivision:\n",
    "    calibrated_science = sci_cut.copy()\n",
    "    calibrated_science_unc = sci_cut_unc.copy()\n",
    "    calibrated_spectra = spectra_cut.copy()\n",
    "    calibrated_spectra_unc = spectra_cut_unc.copy()\n",
    "    \n",
    "    for (key,value), (ukey,uvalue), (fkey,fvalue), (ufkey, ufvalue) in zip(calibrated_science.items(), \n",
    "                calibrated_science_unc.items(), normalized_flat_fields.items(), normalized_flat_fields_unc.items()):\n",
    "        #print(key,ukey,fkey,ufkey)\n",
    "        calibrated_science[key] = value / fvalue \n",
    "        calibrated_science_unc[ukey] = flat_correction_uncertainty(value,uvalue,fvalue,ufvalue)\n",
    "        \n",
    "    for (key,value), (ukey,uvalue), (fkey,fvalue), (ufkey, ufvalue) in zip(calibrated_spectra.items(), \n",
    "                calibrated_spectra_unc.items(), normalized_flat_fields.items(), normalized_flat_fields_unc.items()):\n",
    "        calibrated_spectra[key] = value / fvalue \n",
    "        calibrated_spectra_unc[ukey] = flat_correction_uncertainty(value,uvalue,fvalue,ufvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889fea7-5d20-44fb-ac64-11e09da7c54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Testing\n",
    "if FlatDivision:\n",
    "    value_2d = spectra_dcorr['neon_set1_15s']\n",
    "    value = value_2d.flatten()\n",
    "    print(len(value))\n",
    "    uncert = spectra_dcorr_unc['neon_set1_15s']\n",
    "    plot=False\n",
    "    if plot==True:\n",
    "        #plt.xlim(25000,26000)\n",
    "        #plt.ylim(8200,8500)\n",
    "        plt.scatter(x=np.arange(len(value)), y=value, marker='.', s=1,label='1')\n",
    "        plt.errorbar(x=np.arange(len(value)), y=value, yerr=uncert.flatten(), fmt='.',label='2',capsize=2)\n",
    "        plt.legend()\n",
    "    max_index_flat = np.argmax(uncert)\n",
    "    max_coordinates = np.unravel_index(max_index_flat, uncert.shape)\n",
    "    max_uncert = uncert[max_coordinates]\n",
    "    corr_value = value_2d[max_coordinates]\n",
    "    print(f\"Coords: {max_coordinates}, Value: {corr_value:.3f} +- {max_uncert:.3f}, MagRatio = {np.log10((corr_value/max_uncert))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97a7e5-2f34-465f-bfa5-eca42ccd9931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FlatDivision:\n",
    "    value = calibrated_science['Hbg_sci_set1']\n",
    "    print(np.median(value),np.mean(value), np.std(value))\n",
    "    plt.hist(value.flatten(), bins=50, range=(0, np.max(value)), histtype='step', color='blue')\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4b6a31-3d91-40c3-a5b9-7253671bb848",
   "metadata": {},
   "source": [
    "##### Show Flat Corrected Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694bdfe5-5bb1-4021-be50-dc30cba557ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if FlatDivision:\n",
    "    for key,value in calibrated_science.items():\n",
    "        display_cut_spectrum(value,title=key)\n",
    "        #display_cut_spectrum(calibrated_science_unc[key], title=key+'unc')\n",
    "        #display_cut_spectrum(value - calibrated_science_unc[key], title=key+'+unc')\n",
    "        save_array_to_fits_file(value, path.join('finalized_spectra',f'calibrated_{key}_V2.FIT'))\n",
    "    for key,value in calibrated_spectra.items():\n",
    "        display_cut_spectrum(value,title=key)\n",
    "        #display_cut_spectrum(calibrated_spectra_unc[key], title=key+'unc')\n",
    "        #display_cut_spectrum(value - calibrated_spectra_unc[key], title=key+'+unc')\n",
    "        save_array_to_fits_file(value, path.join('finalized_spectra',f'calibrated_{key}_V2.FIT'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7951f148",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Wavelength Calibration\n",
    "First, we matched spectral lines from our spectrum to emission lines of the actual neon spectrum, and we made a list of pixel positions that lined up with emission wavelengths from the NIST website.\n",
    "We will show how to fit a linear function to a plot of pixel positions vs wavelength to derive a wavelength calibration formula. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c9474-3598-4a8f-b3ef-96834279a5c4",
   "metadata": {},
   "source": [
    "#### Neon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8478b5-9b1d-48c0-b472-3ec2e05d2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_to_wavelength_index(x,slope,b):\n",
    "    return slope*x + b\n",
    "    \n",
    "pix_vs_wavelength = pd.read_csv('Ne_pixels_vs_wavelength.csv')\n",
    "print(pix_vs_wavelength)\n",
    "print(pix_vs_wavelength.columns)\n",
    "pix_vs_wavelength['Ne_x_index_set1'] = pix_vs_wavelength['x_pixel_set1'] - 1.5 #****** starts at 0.5 not 0\n",
    "pix_vs_wavelength['Ne_x_index_set2'] = pix_vs_wavelength['x_pixel_set2'] - 1.5\n",
    "uncertainty = np.ones(len(pix_vs_wavelength['x_pixel_set1']))\n",
    "\n",
    "indices = ['Ne_x_index_set1', 'Ne_x_index_set2']\n",
    "wavelengths = pix_vs_wavelength['wavelength']\n",
    "calibration_stats = {}\n",
    "for index in indices:\n",
    "    x_index = pix_vs_wavelength[index]\n",
    "    slope, intercept = np.polyfit(x_index, wavelengths, 1)\n",
    "    calibration_stats[index] = [slope, intercept]\n",
    "    plt.scatter(x_index, wavelengths, marker='o', s=20, label='Estimated')\n",
    "    plt.plot(x_index, pixel_to_wavelength_index(x_index, slope, intercept), ls='--', lw=1.0, label=index+'Fit')\n",
    "    #plt.errorbar(x_index, pixel_to_wavelength_index(x_index, slope, intercept), uncertainty, fmt='.', label=index+' Uncertainty', ls='solid')\n",
    "    plt.ylabel('Wavelength (nm)')\n",
    "    plt.xlabel('Pixel Index')\n",
    "    #plt.yticks(np.arange(590,671,10))\n",
    "    #plt.xticks(np.arange(40,600,50))\n",
    "    plt.legend()\n",
    "    print(f'{index}==> Slope: {slope:.3f}, Intercept: {intercept:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a362552-d3e7-4c81-ae97-e73301759aec",
   "metadata": {},
   "source": [
    "#### Argon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc7aab-7b46-4d93-ba57-c074829b09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_vs_wavelength = pd.read_csv('Ar_pixels_vs_wavelength.csv').dropna()\n",
    "print(pix_vs_wavelength.columns)\n",
    "pix_vs_wavelength['Ar_x_index_set1'] = pix_vs_wavelength['x_pixel'] - 1.5 \n",
    "pix_vs_wavelength['Ar_x_index_set2'] = pix_vs_wavelength['Ar_x_index_set1'].copy()\n",
    "uncertainty = np.ones(len(pix_vs_wavelength['Ar_x_index_set1']))\n",
    "\n",
    "print(pix_vs_wavelength)\n",
    "indices = ['Ar_x_index_set1', 'Ar_x_index_set2']\n",
    "wavelengths = pix_vs_wavelength['wavelength']\n",
    "for index in indices:\n",
    "    x_index = pix_vs_wavelength[index]\n",
    "    slope, intercept = np.polyfit(x_index, wavelengths, 1)\n",
    "    calibration_stats[index] = [slope, intercept]\n",
    "    plt.scatter(x_index, wavelengths, marker='o', s=20, label='Estimated')\n",
    "    plt.plot(x_index, pixel_to_wavelength_index(x_index, slope, intercept), ls='--', lw=1.0, label=index+'Fit')\n",
    "    #plt.errorbar(x_index, pixel_to_wavelength_index(x_index, slope, intercept), ls='--', lw=1.0, label=index+'Fit')\n",
    "    plt.ylabel('Wavelength (nm)')\n",
    "    plt.xlabel('Pixel Index')\n",
    "    #plt.yticks(np.arange(590,671,10))\n",
    "    #plt.xticks(np.arange(40,600,50))\n",
    "    plt.legend()\n",
    "    print(f'{index}==> Slope: {slope:.3f}, Intercept: {intercept:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f262f-be7a-4203-9279-3bb11308cd1d",
   "metadata": {},
   "source": [
    "## Science (Target Spectra) Gaussian Cutting\n",
    "The idea here is, column by column: \n",
    "1) Find the mean and standard deviation \n",
    "2) Define a cut that selects data points that are above/within a certain threshold range. These data points correspond to the signal from our target, which are the high signal pixels.\n",
    "3) Move all data points that fall outside of this range into a new background array.\n",
    "4) Find the mean and standard deviation of this array and remove any outliers.\n",
    "5) The final signal will be calcualted as follows: \\\n",
    "$\\Large{S' = S - N\\mu_{background}}$,\\\n",
    "where N is the number of pixels in the signal array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4aa67b-a9d6-436c-b907-5001d331f884",
   "metadata": {},
   "source": [
    "#### Signal and Background Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f34dd8-c637-42c4-b293-489340db01a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def histograms_bkg(data,range1,range2,ylim=()):\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.hist(data.flatten(),bins=50, range=range1, alpha=.8, color='red',label='3stds')\n",
    "    plt.hist(data.flatten(),bins=50, range=range2, alpha=.5, color='blue',label='max')\n",
    "    plt.title('background counts - does not show all')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    if ylim != ():\n",
    "        plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84971b4-d301-4856-a611-99c541f3c948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_signal = {key: [] for key in calibrated_science.keys()}\n",
    "target_signal_unc = {key: [] for key in calibrated_science.keys()}\n",
    "background_signal = {key: [] for key in calibrated_science.keys()}\n",
    "background_signal_unc = {key: [] for key in calibrated_science.keys()}\n",
    "\n",
    "for (key,value), (ukey,uvalue), (tkey,tvalue) in zip(calibrated_science.items(), calibrated_science_unc.items(), target_signal.items()):\n",
    "    #Look at statistics of data\n",
    "    medians = np.median(value, axis=0)\n",
    "    means = np.mean(value, axis=0)\n",
    "    stds = np.std(value, axis=0)\n",
    "    print(f\"mean(means1)={np.mean(medians):.3f}, mean(means1)={np.mean(means):.3f}, mean(stds1)={np.mean(stds):.3f}\")\n",
    "    \n",
    "    #Need to look at data above initial background threshold (median)\n",
    "    initial_cut = medians + 100\n",
    "    initial_mask = value > initial_cut\n",
    "    data_cut = np.where(initial_mask, value, np.nan)\n",
    "    data_unc_cut = np.where(initial_mask, uvalue, np.nan)\n",
    "    medians = np.nanmedian(data_cut, axis=0)\n",
    "    means = np.nanmean(data_cut, axis=0)\n",
    "    stds = np.nanstd(data_cut, axis=0)\n",
    "    print(f\"mean(medians2)={np.mean(medians):.3f}, mean(means2)={np.mean(means):.3f}, mean(stds2)={np.mean(stds):.3f}\")\n",
    "    \n",
    "    #Define Second Cut on Target\n",
    "    target_min = means - stds #1\n",
    "    target_max = means + 3*stds #3\n",
    "    target_mask = (data_cut >= target_min) & (data_cut <= target_max)\n",
    "    filtered_data = np.where(target_mask, data_cut, np.nan)\n",
    "    filtered_data_unc = np.where(target_mask, data_unc_cut, np.nan)\n",
    "    #Save to dict\n",
    "    target_signal[key] = filtered_data\n",
    "    target_signal_unc[key] = filtered_data_unc\n",
    "    #print(filtered_data, '\\n\\n\\n\\n', filtered_data_unc)\n",
    "    \n",
    "    #Background Data\n",
    "    bkg_mask_nan = np.isnan(filtered_data)\n",
    "    background = np.where(bkg_mask_nan, value, np.nan) \n",
    "    background_unc = np.where(bkg_mask_nan, uvalue, np.nan) \n",
    "    background = np.where(background == 0, np.nan, background)\n",
    "    # display_cut_spectrum(np.where(bkg_mask_nan, 1, 0),\n",
    "    #    title=key+' background and signal \\n [bckg(red), target(white/cream)]', cmap='Reds')\n",
    "    # display_cut_spectrum(np.where(bkg_mask_nan, value, 0),\n",
    "    #    title=key+' background and signal actual values \\n [bckg(red), target(white/cream)]', cmap='Reds')\n",
    "    \n",
    "    #Background Statistics\n",
    "    medians = np.nanmedian(background, axis=0)\n",
    "    means = np.nanmean(background, axis=0)\n",
    "    stds = np.nanstd(background, axis=0)\n",
    "    maximum = np.max(np.where(np.isnan(background), 0, background))\n",
    "    mu=np.mean(means)\n",
    "    sig=np.std(stds)\n",
    "    print(f\"mean(mediansbkg)={np.mean(medians):.3f}, mean(meansbkg)={mu:.3f}, mean(stdsbkg)={sig:.3f}, max={maximum}\",end='\\n\\n')\n",
    "    #histograms_bkg(background, range1=[mu-sig,mu+3*sig], range2=[-10,maximum], ylim=(0,1000))\n",
    "\n",
    "    #Now define a cut on the background to remove excess noise\n",
    "    bkg_min = 0\n",
    "    bkg_max = means + 1*stds #1\n",
    "    bkg_mask = (background > bkg_min) & (background <= bkg_max)\n",
    "    background_signal[key] = np.where(bkg_mask, background, np.nan)\n",
    "    background_signal_unc[key] = np.where(bkg_mask, background_unc, np.nan)\n",
    "    means = np.nanmean(background, axis=0)\n",
    "    stds = np.nanstd(background, axis=0)\n",
    "    print(f\"mean(mean_background)={np.mean(means):.3f}, mean(stds_background)={np.mean(stds):.3f}\",end='\\n\\n')\n",
    "    display_cut_spectrum(np.where(bkg_mask, 1, 0),        \n",
    "            title=key+' background and signal \\n [bckg(red), target(white/cream)]', cmap='Reds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da1047-c95d-4407-8a80-00e719136543",
   "metadata": {},
   "source": [
    "#### Adjusted Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc6f0a-dba2-491d-ae9b-78d256e2caae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weighted_mean(data,uncertainties,axis):\n",
    "    uncertainties = np.nan_to_num(uncertainties, nan=np.inf)\n",
    "    # Weights\n",
    "    weights = 1 / uncertainties**2\n",
    "    # Weighted Mean\n",
    "    weighted_mean = np.nansum(data * weights, axis=axis) / np.nansum(weights, axis=axis)\n",
    "    # Uncertianty in Mean\n",
    "    uncertainty_mean = np.sqrt(1 / np.nansum(weights, axis=axis))\n",
    "    return weighted_mean, uncertainty_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc391647-6488-48af-bb31-e5ec0bb7b70e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finalized_science = {key: [] for key in calibrated_science.keys()}\n",
    "finalized_science_unc = {key: [] for key in calibrated_science.keys()}\n",
    "\n",
    "for (key,value), (ukey,uvalue), (bkey, bvalue), (ubkey, ubvalue) in zip(target_signal.items(), \n",
    "                 target_signal_unc.items(), background_signal.items(), background_signal_unc.items()):\n",
    "    #print(key,ukey,bkey,ubkey)'\n",
    "    \n",
    "    # Calculate S'\n",
    "    target_sum = np.nansum(value, axis=0)  \n",
    "    target_pixels = np.sum(~np.isnan(value), axis=0) # Number of target pixels per column\n",
    "    #print(target_pixels)\n",
    "    background_mean = weighted_mean(bvalue, ubvalue, axis=0)[0] # mean of each column\n",
    "    #print(background_mean)\n",
    "    finalized_science[key] = target_sum - (target_pixels*background_mean)\n",
    "    \n",
    "    # Calculate uncertainties\n",
    "    target_sum_unc = quadrature_unc(uvalue, axis=0)\n",
    "    #print(np.nansum(target_sum_unc))\n",
    "    background_mean_unc = weighted_mean(bvalue, ubvalue, axis=0)[1]\n",
    "    #print(np.nansum(background_mean_unc))\n",
    "    finalized_science_unc[key] = darks_correction_uncertianty(target_sum_unc, target_pixels*background_mean_unc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac63b5-7821-420a-8e0e-3aa1808d3c48",
   "metadata": {},
   "source": [
    "#### Initial Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fdeaeb-75e5-4be6-a21e-94f168b2cf6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Raw target (without background subtraction)\n",
    "plot = False\n",
    "data = target_signal['Ha_sci_set1']\n",
    "data_unc = target_signal_unc['Ha_sci_set1']\n",
    "y = normalize_to_unity(data, array_uncert=data_unc, return_uncert=False)\n",
    "yerror = normalize_to_unity(data, array_uncert=data_unc, return_uncert=True)\n",
    "x = np.arange(len(y))\n",
    "if plot:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(x,y, color='coral')\n",
    "    plt.yticks(np.arange(0,1.1,.1))\n",
    "    #plt.errorbar(x,y,yerr=yerror, fmt='.',color='coral', alpha=.6)\n",
    "    #plt.xlim(370,400)\n",
    "    #plt.ylim(.2,.5)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cafd7bc-ff1e-4f51-9dbd-fe83c27bab0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot = True\n",
    "ploterr = False\n",
    "data = background_signal['Hbg_sci_set2']\n",
    "data_unc = background_signal_unc['Hbg_sci_set2']\n",
    "y = np.nansum(data, axis=0)\n",
    "yerror = np.sqrt(np.nansum(np.square(data_unc), axis=0))\n",
    "x = np.arange(len(y))\n",
    "if plot:\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(x,y, color='coral')\n",
    "    if ploterr: plt.errorbar(x,y,yerr=yerror, fmt='.',color='coral', alpha=.6)\n",
    "    #plt.xlim(370,400)\n",
    "    #plt.xlim(685,710)\n",
    "    #plt.ylim(.2,.5)\n",
    "    #plt.ylim(.45,.9)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52fe4c-d83d-4200-a74a-55f5ea320eae",
   "metadata": {},
   "source": [
    "If you see a dip in the background that might mean we dont have all the signal. If there is a spike maybe it is emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4ee64c-57df-4641-a7be-d0fc8d32b17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = finalized_science['Ha_sci_set2']\n",
    "data_unc = finalized_science_unc['Ha_sci_set2']\n",
    "\n",
    "y = normalize_to_unity_1d(data, data_unc)[0]\n",
    "yerror = normalize_to_unity_1d(data, data_unc)[1]\n",
    "x = np.arange(len(y))\n",
    "mask = ((x <= 350) | (x >= 438)) & ((x <= 91) | (x >= 111)) & ((x <= 684) | (x >= 710))\n",
    "#mask = ((x >= 200)) & ((x <= 690) | (x >= 710)) & (x > 200)\n",
    "good_data = x > 80\n",
    "degree = 10\n",
    "coefficents = np.polyfit(x=x[mask], y=y[mask], deg=degree)\n",
    "polynomial_fit = np.polyval(coefficents, x[good_data])\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,1,figsize=(8,8))\n",
    "ax1.plot(x,y, color='coral')\n",
    "scatter = ax1.scatter(x,y, color='coral', marker='.', s=10)\n",
    "ax1.plot(x[good_data], polynomial_fit/np.max(polynomial_fit), alpha=0.5)\n",
    "ax1.set_yticks(np.arange(0,1.1,.1))\n",
    "ax1.grid(True)\n",
    "mplcursors.cursor(scatter, hover=False).connect(\n",
    "    \"add\", lambda sel: sel.annotation.set_text(f\"({x[sel.index]}, {y[sel.index]})\")\n",
    ")\n",
    "\n",
    "ax2.plot(x[good_data],y[good_data]/polynomial_fit, marker='.', ms=3,color='salmon')\n",
    "ax2.set_yticks(np.arange(0,1.1,.1))\n",
    "ax2.set_ylim(0,1.1)\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.99, bottom=0.1)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e597c46-5ee6-429d-aba4-321a239d4e05",
   "metadata": {},
   "source": [
    "## Plot Calibrated Spectrum and Science Spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3d851-bb35-4a4b-8764-0aa7f9d02b96",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Key initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eef64e-d608-4c4c-ae05-27158250473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in calibration_stats.items(): print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a942d02-3dfd-4930-9b3d-e41f5561d4a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### Normalize Spectra to be capped at 1 (for visualizing separately)\n",
    "normalized_spectra = {'Ne_set1' : normalize_to_unity(calibrated_spectra['Ne_spect_set1'], ctype='median'),\n",
    "                      'Ne_set2' : normalize_to_unity(calibrated_spectra['Ne_spect_set2'], ctype='median'),\n",
    "                      'Ha_set1' : normalize_to_unity_1d(finalized_science['Ha_sci_set1'], finalized_science_unc['Ha_sci_set1'])[0],\n",
    "                      'Ha_set2' : normalize_to_unity_1d(finalized_science['Ha_sci_set2'], finalized_science_unc['Ha_sci_set2'])[0],\n",
    "                      'Ar_set1' : normalize_to_unity(calibrated_spectra['Ar_spect_set1'], ctype='median'),\n",
    "                      'Ar_set2' : normalize_to_unity(calibrated_spectra['Ar_spect_set2'], ctype='median'),\n",
    "                      'Hbg_set1' : normalize_to_unity_1d(finalized_science['Hbg_sci_set1'], finalized_science_unc['Hbg_sci_set1'])[0],\n",
    "                      'Hbg_set2' : normalize_to_unity_1d(finalized_science['Hbg_sci_set2'], finalized_science_unc['Hbg_sci_set2'])[0],\n",
    "                     }\n",
    "normalized_spectra_unc = {'Ne_set1' : normalize_to_unity(calibrated_spectra['Ne_spect_set1'], ctype='median', return_uncert=True, \n",
    "                                      array_uncert=calibrated_spectra_unc['Ne_spect_set1']),\n",
    "                          'Ne_set2' : normalize_to_unity(calibrated_spectra['Ne_spect_set2'], ctype='median', return_uncert=True, \n",
    "                                      array_uncert=calibrated_spectra_unc['Ne_spect_set2']),\n",
    "                          'Ha_set1' : normalize_to_unity_1d(finalized_science['Ha_sci_set1'], finalized_science_unc['Ha_sci_set1'])[1],\n",
    "                          'Ha_set2' : normalize_to_unity_1d(finalized_science['Ha_sci_set2'], finalized_science_unc['Ha_sci_set2'])[1],\n",
    "                          'Ar_set1' : normalize_to_unity(calibrated_spectra['Ar_spect_set1'], ctype='median', return_uncert=True, \n",
    "                                      array_uncert=calibrated_spectra_unc['Ar_spect_set1']),\n",
    "                          'Ar_set2' : normalize_to_unity(calibrated_spectra['Ar_spect_set2'], ctype='median', return_uncert=True, \n",
    "                                      array_uncert=calibrated_spectra_unc['Ar_spect_set2']),\n",
    "                          'Hbg_set1' : normalize_to_unity_1d(finalized_science['Hbg_sci_set1'], finalized_science_unc['Hbg_sci_set1'])[1],\n",
    "                          'Hbg_set2' : normalize_to_unity_1d(finalized_science['Hbg_sci_set2'], finalized_science_unc['Hbg_sci_set2'])[1],\n",
    "                          }\n",
    "calibration_indices = {'Ne_set1' : 'Ne_x_index_set1',\n",
    "                      'Ne_set2' : 'Ne_x_index_set2',\n",
    "                      'Ha_set1' : 'Ne_x_index_set1',\n",
    "                      'Ha_set2' : 'Ne_x_index_set2',\n",
    "                      'Ar_set1' : 'Ar_x_index_set1',\n",
    "                      'Ar_set2' : 'Ar_x_index_set2',\n",
    "                      'Hbg_set1' : 'Ar_x_index_set1',\n",
    "                      'Hbg_set2' : 'Ar_x_index_set2',\n",
    "                     }\n",
    "label_list = ['Neon Set 1', 'Neon Set 2', 'H$\\\\alpha$ Set 1', 'H$\\\\alpha$ Set 2', 'Ar Set 1', 'Ar Set 2', 'H$\\\\beta , \\\\gamma$ Set 1', 'H$\\\\beta , \\\\gamma$ Set 2']\n",
    "labels = {}\n",
    "for label, (key,value) in zip(label_list, calibration_indices.items()): labels[key] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c10a085-a4d0-4eb5-9bea-064a6d763754",
   "metadata": {},
   "source": [
    "#### Single Plot\n",
    "All you have to do is change the \"spectrum_label\" and you can also decide if you want to plot the error bars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c25bf-b189-45d6-bc48-8f4aa81682fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spectrum_label,index_set in calibration_indices.items():\n",
    "    print(f\"Label: {spectrum_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb459e-ad91-4e93-9952-cc6058f1226e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spectrum_label = 'Ha_set2'\n",
    "plt_error=False\n",
    "\n",
    "index_set = calibration_indices[spectrum_label]\n",
    "spectrum_index = np.arange(len(normalized_spectra[spectrum_label])) #pixel index\n",
    "spectrum_wavelength_index = pixel_to_wavelength_index(spectrum_index, *calibration_stats[index_set]) #pixel index --> wavelength index\n",
    "'''\n",
    "#Find Peaks\n",
    "peaks, _ = find_peaks(normalized_spectra[spectrum_label], height=(0.0,.4))  # Customize height threshold if needed\n",
    "#print(peaks)\n",
    "for peak in peaks:\n",
    "    plt.annotate(f'{spectrum_wavelength_index[peak]:.1f} nm', \n",
    "                 (spectrum_wavelength_index[peak], normalized_spectra[spectrum_label][peak]), \n",
    "                 xytext=(0, 4), textcoords='offset points', ha='center', fontsize=7, color='red')\n",
    "#'''\n",
    "color = 'coral'\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(spectrum_wavelength_index, normalized_spectra[spectrum_label], ms=5, c=color, label=labels[spectrum_label])\n",
    "if plt_error:\n",
    "    ax.errorbar(spectrum_wavelength_index, normalized_spectra[spectrum_label], yerr=normalized_spectra_unc[spectrum_label], \n",
    "                 fmt='.', c=color, label=' Uncertainty', alpha=.5)\n",
    "ax.set_ylabel('Intensity')\n",
    "ax.set_xlabel('Wavelength (nm)')\n",
    "ax.set_ylim(0.0,1.0)\n",
    "ax.set_yticks(ticks=np.arange(0.0,1.1,0.1))\n",
    "#ax.set_xlim(np.min(spectrum_wavelength_index), np.max(spectrum_wavelength_index))\n",
    "#ax.xaxis.set_major_locator(MaxNLocator(integer=True, prune='both', nbins=150))\n",
    "#ax.set_xticks(ticks=np.arange(np.min(spectrum_wavelength_index), np.max(spectrum_wavelength_index),5))#(ax.get_xticks())\n",
    "#ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "#plt.xlim(600,680)\n",
    "\n",
    "#Zoom in to a regiOn\n",
    "#alpha:\n",
    "#ax.set_xlim(655, 657.5)\n",
    "#ax.set_ylim(0.3,0.7)\n",
    "#beta:\n",
    "#plt.xlim(485,487)\n",
    "#plt.ylim(0.25,0.6)\n",
    "#gamma:\n",
    "#plt.xlim(433,436)\n",
    "#plt.ylim(0.1,0.6)\n",
    "\n",
    "#ax.set_xlim(490,500)\n",
    "#ax.set_ylim(.00,.2)\n",
    "\n",
    "#ax.set_xlim(686,688)\n",
    "#ax.set_ylim(.4,.9)\n",
    "#plt.xlim(685,690)\n",
    "#plt.ylim(0.2,0.7)\n",
    "\n",
    "#Plot Balmer Lines:\n",
    "#plt.vlines(x=656.3, ymin=0, ymax=1, color='r', label='H$\\\\alpha$')\n",
    "#plt.vlines(x=486.1, ymin=0, ymax=1, color='blue', label='H$\\\\beta$ (486.1nm)', alpha=.5)\n",
    "#plt.vlines(x=434, ymin=0, ymax=1, color='purple', label='H$\\\\gamma$ (434 nm)',  alpha=.5)\n",
    "#plt.vlines(x=687, ymin=0, ymax=1, color='lightgreen', label='Telluric O2')\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.90, bottom=0.1)\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec28410-8e49-4826-bdcb-c2b476aedbbc",
   "metadata": {},
   "source": [
    "#### Stacked Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e40c39-ae24-4935-928c-7b6f94e5c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View Stacked Spectra\n",
    "plot_error = False\n",
    "spectrum_labels = ['Hbg_set1','Hbg_set2'] #['Ha_set1','Ha_set2','Hbg_set1', 'Hbg_set2'] \n",
    "index_sets=[]\n",
    "for i in range(len(spectrum_labels)):\n",
    "    index_sets.append(calibration_indices[spectrum_labels[i]])\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "for (s_label, i_set) in zip(spectrum_labels, index_sets):\n",
    "    spectrum_index = np.arange(len(normalized_spectra[s_label]))\n",
    "    spectrum_wavelength_index = pixel_to_wavelength_index(spectrum_index, *calibration_stats[i_set])\n",
    "    line, = ax.plot(spectrum_wavelength_index, normalized_spectra[s_label], label=s_label.replace('_',' ').replace('b',f'$\\\\beta$').replace('g','$\\\\gamma $').replace('set','Set '))\n",
    "    line_color = line.get_color()\n",
    "    if plot_error:\n",
    "        ax.errorbar(spectrum_wavelength_index, normalized_spectra[s_label], normalized_spectra_unc[s_label], \n",
    "                fmt='.', label=s_label+' Uncertainty', color=line_color, alpha=.8)\n",
    "'''\n",
    "#Find Peaks\n",
    "for spectrum_label in spectrum_labels:\n",
    "    peaks, _ = find_peaks(normalized_spectra[spectrum_label], height=(.3,.4))  # Customize height threshold if needed\n",
    "    print(peaks)\n",
    "    for peak in peaks:\n",
    "        plt.annotate(f'{spectrum_wavelength_index[peak]:.1f} nm', \n",
    "                     (spectrum_wavelength_index[peak], normalized_spectra[spectrum_label][peak]), \n",
    "                     xytext=(0, 4), textcoords='offset points', ha='center', fontsize=7, color='red')\n",
    "'''\n",
    "plt.title('Hydrogen $\\\\beta , \\\\gamma$ Absorption Lines')\n",
    "plt.ylabel('Intensity')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.yticks(np.arange(0.0,1.1,0.1))\n",
    "#plt.xticks(np.arange(420,500,5),rotation=90)\n",
    "#plt.xticks(np.arange(420,500,5),rotation=90)\n",
    "\n",
    "#Zoom in to a regiOn\n",
    "#alpha:\n",
    "#ax.set_xlim(655.5, 658)\n",
    "#ax.set_ylim(0.25,0.55)\n",
    "#beta:\n",
    "#plt.xlim(484,488)\n",
    "#plt.ylim(0.3,0.9)\n",
    "#gamma:\n",
    "#plt.xlim(433,436.5)\n",
    "#plt.ylim(0.05,0.7)\n",
    "#O2:\n",
    "#plt.xlim(685,690)\n",
    "#plt.ylim(0.2,0.7)\n",
    "\n",
    "#Plot Balmer Lines:\n",
    "#plt.vlines(x=656.3, ymin=0, ymax=1, color='r', label='H$\\\\alpha$')\n",
    "plt.vlines(x=486.1, ymin=0, ymax=1, color='blue', label='H$\\\\beta$ (486.1nm)', alpha=.5)\n",
    "plt.vlines(x=434, ymin=0, ymax=1, color='purple', label='H$\\\\gamma$ (434 nm)',  alpha=.5)\n",
    "#plt.vlines(x=687, ymin=0, ymax=1, color='lightgreen', label='Telluric O2')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.90, bottom=0.1)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7d5bd5-0895-40d7-ae0e-fac9453d43f1",
   "metadata": {},
   "source": [
    "#### Aligning H alpha data with Telluric Oxygen Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cbe63e-d870-4064-a6d7-d0d18db2dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_label = 'Ha_set1'\n",
    "index_set = calibration_indices[spectrum_label]\n",
    "spectrum_index = np.arange(len(normalized_spectra[spectrum_label])) #pixel index\n",
    "wavelengths = pixel_to_wavelength_index(spectrum_index, *calibration_stats[index_set]) #pixel index --> wavelength index\n",
    "spectrum = normalized_spectra[spectrum_label]\n",
    "peaks, properties = find_peaks(spectrum, height=(.2,.5))  # Set height threshold if needed\n",
    "#print(peaks, pixel_to_wavelength_index(peaks, *calibration_stats[index_set]), properties[\"peak_heights\"])\n",
    "fig, ax = plt.subplots(1,2,figsize=(12,5))\n",
    "#ax[0].scatter(pixel_to_wavelength_index(peaks, *calibration_stats[index_set]), properties[\"peak_heights\"], marker='*', s=30, c='r')\n",
    "#ax[0].plot(wavelengths, spectrum, marker='.', alpha=.5, label='initial')\n",
    "ax[0].set_xlim(685,690)\n",
    "ax[0].set_ylim(.3,.65)\n",
    "ax[0].vlines(x=687, ymin=0, ymax=1, color='lightgreen', label='Telluric O2')\n",
    "ha1_shift = 687 - pixel_to_wavelength_index(693, *calibration_stats[index_set])\n",
    "wavelengths_adj_ha1 = ha1_shift + pixel_to_wavelength_index(spectrum_index, *calibration_stats[index_set])\n",
    "ax[0].plot(wavelengths_adj_ha1, spectrum, marker='.', alpha=.5, label='adjusted')\n",
    "ax[0].grid(True)\n",
    "ax[0].legend()\n",
    "\n",
    "spectrum_label = 'Ha_set2'\n",
    "index_set = calibration_indices[spectrum_label]\n",
    "spectrum_index = np.arange(len(normalized_spectra[spectrum_label])) #pixel index\n",
    "wavelengths = pixel_to_wavelength_index(spectrum_index, *calibration_stats[index_set]) #pixel index --> wavelength index\n",
    "spectrum = normalized_spectra[spectrum_label]\n",
    "peaks, properties = find_peaks(spectrum, height=(.2,.5))  # Set height threshold if needed\n",
    "print(peaks, pixel_to_wavelength_index(peaks, *calibration_stats[index_set]), properties[\"peak_heights\"])\n",
    "ax[1].scatter(pixel_to_wavelength_index(peaks, *calibration_stats[index_set]), properties[\"peak_heights\"], marker='*', s=30, c='r')\n",
    "ax[1].plot(wavelengths, spectrum, marker='.', alpha=.5, label='initial')\n",
    "pos = 686\n",
    "ax[1].scatter(pixel_to_wavelength_index(pos,*calibration_stats[index_set]), spectrum[pos], c='green')\n",
    "ax[1].set_xlim(685,690)\n",
    "ax[1].set_ylim(.3,.65)\n",
    "ax[1].vlines(x=687, ymin=0, ymax=1, color='lightgreen', label='Telluric O2')\n",
    "ha2_shift = 687 - pixel_to_wavelength_index(pos, *calibration_stats[index_set])\n",
    "wavelengths_adj_ha2 = pixel_to_wavelength_index(spectrum_index, *calibration_stats[index_set]) + ha2_shift\n",
    "ax[1].plot(wavelengths_adj_ha2, spectrum, marker='.', alpha=.5,) #label='adjusted')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(wavelengths_adj_ha1, normalized_spectra['Ha_set1'], label='Ha Set 1')\n",
    "plt.plot(wavelengths_adj_ha2, normalized_spectra['Ha_set2'], label='Ha Set 2')\n",
    "plt.xlim(685,690)\n",
    "plt.ylim(.2,.7)\n",
    "plt.figure(figsize=(12,3))\n",
    "plt.plot(wavelengths_adj_ha1, normalized_spectra['Ha_set1'], label='Ha Set 1')\n",
    "plt.plot(wavelengths_adj_ha2, normalized_spectra['Ha_set2'], label='Ha Set 2')\n",
    "plt.axvline(656.3)\n",
    "plt.xlim(653, 660)\n",
    "plt.ylim(0.3,0.8)\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.99, bottom=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab687019-2b66-40d1-8584-907acd66fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_wavelengths = {'Ha_set1' : wavelengths_adj_ha1,\n",
    "                        'Ha_set2' : wavelengths_adj_ha2\n",
    "                       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e78a1d5-1fed-40f4-81ea-ac34f1b3b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View Stacked Spectra\n",
    "plot_error = False\n",
    "spectrum_labels = ['Ha_set1','Ha_set2']\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "for label, (spectrum_key, spectrum), (wavelengths_key, wavelengths) in zip(spectrum_labels, normalized_spectra.items(), adjusted_wavelengths.items()):\n",
    "    line, = ax.plot(wavelengths, normalized_spectra[label], label=label.replace('_',' ').replace('a',f'$\\\\alpha$').replace('set','Set '))\n",
    "    line_color = line.get_color()\n",
    "    if plot_error:\n",
    "        ax.errorbar(wavelengths, normalized_spectra[label], normalized_spectra_unc[label], \n",
    "                fmt='.', label=s_label+' Uncertainty', color=line_color, alpha=.8)\n",
    "plt.title('Hydrogen $\\\\alpha$ Absorption Line')\n",
    "plt.ylabel('Intensity')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.yticks(np.arange(0.0,1.1,0.1))\n",
    "plt.xticks(np.arange(610,730,10),rotation=0)\n",
    "#plt.xticks(np.arange(420,500,5),rotation=90)\n",
    "plt.ylim(.3,1)\n",
    "#Zoom in to a regiOn\n",
    "#alpha:\n",
    "#ax.set_xlim(655.5, 658)\n",
    "#ax.set_ylim(0.25,0.55)\n",
    "#beta:\n",
    "#plt.xlim(484,488)\n",
    "#plt.ylim(0.3,0.9)\n",
    "#gamma:\n",
    "#plt.xlim(433,436.5)\n",
    "#plt.ylim(0.05,0.7)\n",
    "#O2:\n",
    "#plt.xlim(685,690)\n",
    "#plt.ylim(0.2,0.7)\n",
    "\n",
    "#Plot Balmer Lines:\n",
    "plt.vlines(x=656.3, ymin=0, ymax=1, color='r', label='H$\\\\alpha$ (656.3 nm)', alpha=.5)\n",
    "#plt.vlines(x=486.1, ymin=0, ymax=1, color='cyan', label='H$\\\\beta$')\n",
    "#plt.vlines(x=434, ymin=0, ymax=1, color='blue', label='H$\\\\gamma$')\n",
    "plt.vlines(x=687, ymin=0, ymax=1, color='lightgreen', label='Telluric O2 (687 nm)', alpha=.8)\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8671a683-b620-4c9e-9489-525697bf0a1d",
   "metadata": {},
   "source": [
    "## Gaussian Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e84d1d-a62c-40bf-8d5a-2d41d302008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelength_to_pixel_index(wavelength, slope, b):\n",
    "    return (wavelength - b) / slope\n",
    "def gaussian(x, a, mu, sigma):\n",
    "    return a * np.exp(-((x - mu)**2) / (2 * sigma**2))\n",
    "def double_gaussian(wavelength, A1, mu1, sigma1, A2, mu2, sigma2):\n",
    "    return (\n",
    "        A1 * np.exp(-(wavelength - mu1) ** 2 / (2 * sigma1 ** 2)) +\n",
    "        A2 * np.exp(-(wavelength - mu2) ** 2 / (2 * sigma2 ** 2))\n",
    "    )\n",
    "def doppler_shift_function(mu1, mu2, lambda_rest):\n",
    "    '''\n",
    "    Rest Wavelength in nm\n",
    "    mu1 and mu2 in nm\n",
    "    returns in km/s\n",
    "    '''\n",
    "    c = 3e8\n",
    "    delta_lambda = (mu2 - mu1)\n",
    "    return (c * delta_lambda) / (lambda_rest * 1e3)\n",
    "rest_wavelengths = {'Ha' : 656.3,\n",
    "                    'Hb' : 486.1,\n",
    "                    'Hg' : 434}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34da2c5-3593-4584-832a-b47052a20ddd",
   "metadata": {},
   "source": [
    "#### Choose Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e162c-9d73-4cf6-8d79-d711002a79f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spectrum_label = 'Ha_set1'\n",
    "label = labels[spectrum_label]\n",
    "lambda_rest = rest_wavelengths['Ha']\n",
    "index_set = calibration_indices[spectrum_label]\n",
    "spectrum_index = np.arange(len(normalized_spectra[spectrum_label])) #pixel index\n",
    "\n",
    "spectrum_wavelength_index = pixel_to_wavelength_index(spectrum_index, *calibration_stats[index_set]) #pixel index --> wavelength index\n",
    "\n",
    "plt.figure(figsize=(7, 2))\n",
    "plt.plot(spectrum_wavelength_index, normalized_spectra[spectrum_label], label=spectrum_label+' Calibrated Spectrum', ms=5, c=color)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c571dfe7-e984-4805-9699-cf9a870915ed",
   "metadata": {},
   "source": [
    "#### Set Mask and Intensity Offset\n",
    "Select the wavelength range with the mask, and set the offset so that the base of the gaussian is around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee05ee9-f0ca-44c5-b1d3-0f8475a719ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set wavelength range with the mask\n",
    "# adjust offset until the base of the gaussian is around 0.0\n",
    "wide_view = False # For fining the mask\n",
    "if spectrum_label == 'Ha_set1':\n",
    "    wavelength_mask = [656.3,657.3]\n",
    "    offset = -.74 \n",
    "elif spectrum_label == 'Ha_set2':\n",
    "    wavelength_mask = [655.6,657]\n",
    "    offset = -0.54 + .14 -.175\n",
    "elif spectrum_label == 'Hbg_set1':\n",
    "    wavelength_mask = [485.5,486.6]\n",
    "    offset = -.65 +.17\n",
    "elif spectrum_label == 'Hbg_set2':\n",
    "    wavelength_mask = [485.5,486.6]\n",
    "    offset = -.47\n",
    "## Hgamma set 1:\n",
    "# wavelength_mask = [433.9,435.05]\n",
    "# offset = -.32\n",
    "## Hgamma set 2:\n",
    "# wavelength_mask = [485.5,486.6]\n",
    "# offset = -.32\n",
    "\n",
    "mask = (spectrum_wavelength_index >= wavelength_mask[0]) & (spectrum_wavelength_index <= wavelength_mask[1])\n",
    "wavelengths = spectrum_wavelength_index[mask]\n",
    "spectrum = normalized_spectra[spectrum_label][mask] + offset\n",
    "spectrum_unc = normalized_spectra_unc[spectrum_label][mask]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "color = 'coral'\n",
    "plt.plot(wavelengths, spectrum, ms=5, c=color)\n",
    "plt.errorbar(wavelengths, spectrum, yerr=spectrum_unc, xerr=0.05,\n",
    "            fmt='.', c=color, label=label, alpha=.5)\n",
    "plt.title('Zoomed In Feature')\n",
    "plt.ylabel('Intensity')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "if wide_view: \n",
    "    tick_separation = .5\n",
    "    m=math.floor(wavelength_mask[0] / 0.5) * 0.5\n",
    "    M=math.ceil(wavelength_mask[1] / 0.5) * 0.5\n",
    "else: \n",
    "    tick_separation = 0.05\n",
    "    m=math.floor(wavelength_mask[0] / 0.1) * 0.1\n",
    "    M=math.ceil(wavelength_mask[1] / 0.1) * 0.1\n",
    "plt.xticks(np.arange(m,M,tick_separation), rotation=90, size=9)\n",
    "if not wide_view: plt.yticks(np.arange(round(np.min(spectrum),2),np.max(spectrum)+.01,.01, ))\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper center')\n",
    "plt.tight_layout\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eedf68b-6610-4e34-96b6-2b95c6dd5cec",
   "metadata": {},
   "source": [
    "#### Plot of Double Gaussian Fit\n",
    "Make good initial guesses by looking at the plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659145c-c354-4378-938c-47c5e4a002b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial guesses for fitting (A1, mu1, sigma1, A2, mu2, sigma2)\n",
    "if spectrum_label == 'Ha_set1':\n",
    "    initial_guess = [-0.315, 656.55, 0.2, -0.34, 657., 0.2]\n",
    "elif spectrum_label == 'Ha_set2':\n",
    "    initial_guess = [-0.16, 656.05, 0.2, -0.175, 656.456, 0.2]\n",
    "elif spectrum_label == 'Hbg_set1':\n",
    "    initial_guess = [-0.155, 485.95, 0.2, -0.175, 486.2, 0.2]\n",
    "elif spectrum_label == 'Hbg_set2':\n",
    "    initial_guess = [-0.150, 485.95, 0.2, -0.15, 486.2, 0.2]\n",
    "#Hgamma set 1:\n",
    "# initial_guess = [-0.12, 434.4, 0.1, -0.12, 434.6, 0.1]\n",
    "\n",
    "# Fit the double Gaussian model to the data\n",
    "popt, pcov = curve_fit(double_gaussian, wavelengths, spectrum, p0=initial_guess,\n",
    "                      sigma=spectrum_unc, absolute_sigma=True)\n",
    "\n",
    "# Extract the fitted parameters\n",
    "A1_fit, mu1_fit, sigma1_fit, A2_fit, mu2_fit, sigma2_fit = popt\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "#plt.plot(wavelengths, spectrum, label=spectrum_label+' Calibrated Spectrum', ms=5, c=color)\n",
    "plt.errorbar(wavelengths, spectrum, yerr=spectrum_unc, ls='-',\n",
    "            fmt='.', c=color, label=label+' Uncertainty', alpha=.5)\n",
    "xrange = np.linspace(np.min(wavelengths), np.max(wavelengths), 10000)\n",
    "plt.plot(xrange, double_gaussian(xrange, *popt), 'r-', label='Fitted model', alpha=.7)\n",
    "plt.axvline(mu1_fit, linestyle='--', label=f'Peak 1 ($\\lambda$ = {mu1_fit:.2f} nm)')\n",
    "plt.axvline(mu2_fit, linestyle='--', label=f'Peak 2 ($\\lambda$ = {mu2_fit:.2f} nm)')\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Amplitude (dimensionless)')\n",
    "plt.title('Double Gaussian Fit to Hydrogen Alpha')\n",
    "#plt.axvline(mu1_fit-sigma1_fit, linestyle='--', label=f'$\\mu1 - \\sigma1$',alpha=.4)\n",
    "#plt.axvline(mu2_fit+sigma2_fit, linestyle='--', label=f'$\\mu2 + \\sigma2$',alpha=.4)\n",
    "#plt.xticks(np.arange(653,661,0.25), rotation=90, size=8)\n",
    "plt.xlim(wavelength_mask)\n",
    "plt.legend()\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "plt.grid(True)\n",
    "print(f\"Fitted Parameters:\")\n",
    "print(f\"Peak 1: A1 = {A1_fit:.2f}, mu1 = {mu1_fit:.2f}, sigma1 = {sigma1_fit:.2f}\")\n",
    "print(f\"Peak 2: A2 = {A2_fit:.2f}, mu2 = {mu2_fit:.2f}, sigma2 = {sigma2_fit:.2f}\")\n",
    "lambda_center = abs(mu1_fit + mu2_fit)/2\n",
    "dlambda = lambda_center - lambda_rest\n",
    "print(f\"Lambda Center: {lambda_center:.3f} which is {dlambda:.3f} away from rest\")\n",
    "\n",
    "print(f\"\\nDoppler Shift (from 656.3 nm): {doppler_shift_function(mu1_fit, mu2_fit, lambda_rest=lambda_rest):.2f} km/s\")\n",
    "print(f\"Doppler Shift (from lambda center): {doppler_shift_function(mu1_fit, mu2_fit, lambda_rest=lambda_center):.2f} km/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdada28-ae20-4665-8d49-71d56b79de9c",
   "metadata": {},
   "source": [
    "### Chi Squared and Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342da760-8716-4d4f-b099-dd266727487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_statistics(inputs, actual, actual_uncertainty, popt):\n",
    "    '''\n",
    "    Returns: residuals, chi_squared, reduced_chi_square, p_value\n",
    "    '''\n",
    "    residuals = spectrum - double_gaussian(wavelengths, *popt)\n",
    "    chi_squared = np.sum((residuals / spectrum_unc)**2)\n",
    "    nu = (len(wavelengths) - len(popt))\n",
    "    reduced_chi_square = chi_squared / nu\n",
    "    p_value = 1 - chi2.cdf(chi_squared, nu)\n",
    "    return residuals, chi_squared, reduced_chi_square, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e44c8-5b70-47ea-b69e-631a445d3358",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = spectrum - double_gaussian(wavelengths, *popt)\n",
    "chi_squared = np.sum((residuals / spectrum_unc)**2)\n",
    "nu = (len(wavelengths) - len(popt))\n",
    "reduced_chi_square = chi_squared / nu\n",
    "p_value = 1 - chi2.cdf(chi_squared, nu)\n",
    "print(f\"Chi-Square: {chi_squared:.2f}\")\n",
    "print(f\"Reduced chi-square: {reduced_chi_square:.2f}\")\n",
    "print(f\"P-Value: {p_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc00be0c-6c7f-40ae-8366-e7dcb8bb8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(wavelengths, residuals, yerr=spectrum_unc, fmt='.', label='Residuals', color='blue', ecolor='red', capsize=3)\n",
    "plt.axhline(0, color='black', linestyle='--', label='Zero Residual Line')\n",
    "plt.xlabel('Wavelength')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Plot')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc955052-16cb-4e4d-aec1-a58b5ff09492",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba68f5-5958-4cec-bf1b-a74712a0361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monte Carlo\n",
    "np.random.seed(13) \n",
    "num_simulations = 200\n",
    "wavelength_shifts = []\n",
    "wavelength_unc = np.full_like(wavelengths, 0.05)\n",
    "popt_array = np.zeros((num_simulations, 6))\n",
    "\n",
    "for _ in range(num_simulations):\n",
    "    try:\n",
    "        #Perturbation\n",
    "        perturbed_spectrum = spectrum + np.random.normal(0, spectrum_unc) \n",
    "        perturbed_wavelengths = wavelengths + np.random.normal(0, wavelength_unc)\n",
    "        #Estimate Params\n",
    "        popt, pcov = curve_fit(double_gaussian, perturbed_wavelengths, perturbed_spectrum, \n",
    "                               p0=initial_guess, sigma=spectrum_unc, absolute_sigma=True, maxfev=10000)\n",
    "\n",
    "        #Find doppler shift\n",
    "        popt_array[_, :] = popt\n",
    "        doppler_shift = popt[4] - popt[1]\n",
    "        wavelength_shifts.append(doppler_shift)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during fit: {e}\")\n",
    "        continue  # Skip this iteration if the fit fails\n",
    "print(f\"Iterations Ran: {len(wavelength_shifts)}\")\n",
    "\n",
    "# Calculate Results\n",
    "popt_means = np.mean(popt_array, axis=0)\n",
    "mu1_mean = popt_means[1]\n",
    "mu2_mean = popt_means[4]\n",
    "wavelength_shifts = np.array(wavelength_shifts)\n",
    "mean_shift = np.mean(wavelength_shifts)\n",
    "std_shift = np.std(wavelength_shifts)\n",
    "mean_doppler_shift_velocity = doppler_shift_function(mu2=mean_shift, mu1=0, lambda_rest=lambda_rest)\n",
    "mean_doppler_shift_uncertainty = doppler_shift_function(mu2=std_shift, mu1=0, lambda_rest=lambda_rest)\n",
    "\n",
    "print(f\"Mean Wavelength Shift: {mean_shift:.3f} nm\")\n",
    "print(f\"Uncertainty in Wavelength Shift: {std_shift:.3f} nm\")\n",
    "print(f\"Mean Doppler Shift: {mean_doppler_shift_velocity:.3f} km/s\")\n",
    "print(f\"Uncertainty in Doppler Shift: {mean_doppler_shift_uncertainty:.3f} km/s\")\n",
    "\n",
    "# Plot distribution of Doppler shifts\n",
    "bins = np.arange(min(wavelength_shifts), max(wavelength_shifts), 0.02)  # Adjust bin width if necessary\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(wavelength_shifts, bins=bins, alpha=0.75, color='blue', label='Doppler Shifts')\n",
    "plt.axvline(mean_shift, color='red', linestyle='--', label=f'Mean: {mean_shift:.3f}')\n",
    "plt.axvline(mean_shift + std_shift, color='red', linestyle='--', label=f'Mean+std: {mean_shift + std_shift:.3f}', alpha=.3)\n",
    "plt.axvline(mean_shift - std_shift, color='red', linestyle='--', label=f'Mean-std: {mean_shift - std_shift:.3f}', alpha=.3)\n",
    "plt.xlabel('Doppler Shift')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Monte Carlo Simulation of Doppler Shift')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "print(mu1_mean, mu2_mean)\n",
    "print(popt_means[2],popt_means[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259a05e-2a98-491f-b534-ea4fafda6ece",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulation (Streamlined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83639a-7a7e-4ad5-8044-e920a0e49d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guesses = {'Ha_set1': [-0.315, 656.55, 0.2, -0.34, 657., 0.2], #[-0.315, 656.55, 0.2, -0.34, 657., 0.2]\n",
    "                   'Ha_set2': [-0.16, 656.05, 0.2, -0.175, 656.456, 0.2],\n",
    "                   'Hbg_set1': [-0.155, 485.95, 0.2, -0.175, 486.2, 0.2],\n",
    "                   'Hbg_set2': [-0.150, 485.95, 0.2, -0.15, 486.2, 0.2]\n",
    "                   }\n",
    "masks = {'Ha_set1' : [656.3,657.3],\n",
    "         'Ha_set2' : [655.6,657],\n",
    "         'Hbg_set1' : [485.5,486.6],\n",
    "         'Hbg_set2' : [485.5,486.6]\n",
    "        }\n",
    "offsets = {'Ha_set1' : -0.74,\n",
    "           'Ha_set2' : -0.54 + .14 -.175,\n",
    "           'Hbg_set1' : -0.65 +.17,\n",
    "           'Hbg_set2' : -0.47\n",
    "          }\n",
    "rest_wavelengths = {'Ha_set1' : 656.3,\n",
    "                    'Ha_set2' : 656.3,\n",
    "                    'Hbg_set1' : 486.1,\n",
    "                    'Hbg_set2' : 486.1\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07923301-0027-4445-b856-35e71b158a9d",
   "metadata": {},
   "source": [
    "Steps:\n",
    "For each absorption line\n",
    "1) select proper masks and offsets to apply\n",
    "2) perturb the spectrum and wavelengths\n",
    "3) find optimal parameters\n",
    "4) loop 200x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0da4d5-555e-4b0b-b239-e70b878a6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 20000 # max number\n",
    "target_number = 500\n",
    "monte_carlo_stats = {}\n",
    "\n",
    "for (guess_key, guess), (mask_key, mask), (offset_key, offset), (lambda_key, rest_wavelength) in zip(initial_guesses.items(), masks.items(), offsets.items(), rest_wavelengths.items()):\n",
    "    np.random.seed(13) \n",
    "    \n",
    "    #Set Spectra Data\n",
    "    lambda_rest = rest_wavelength\n",
    "    index_set = calibration_indices[guess_key]\n",
    "    spectrum_index = np.arange(len(normalized_spectra[guess_key])) #pixel index\n",
    "    spectrum_wavelength_index = pixel_to_wavelength_index(spectrum_index, *calibration_stats[index_set]) #pixel index --> wavelength index\n",
    "    spectrum = normalized_spectra[guess_key]\n",
    "    spectrum_unc = normalized_spectra_unc[guess_key]\n",
    "    \n",
    "    #Adjust Data\n",
    "    wavelengths_mask = (spectrum_wavelength_index >= mask[0]) & (spectrum_wavelength_index <= mask[1])\n",
    "    wavelengths = spectrum_wavelength_index[wavelengths_mask]\n",
    "    spectrum = spectrum[wavelengths_mask] + offset\n",
    "    spectrum_unc = spectrum_unc[wavelengths_mask]\n",
    "    \n",
    "    #Initialize Monte Carlo Parameters\n",
    "    wavelength_shifts = []\n",
    "    wavelength_unc = np.full_like(wavelengths, 0.02)\n",
    "    popt_array = np.zeros((num_simulations, 6)) #N x 6 parameters array\n",
    "    \n",
    "    #Monte Carlo Simulation\n",
    "    for i in range(num_simulations):\n",
    "        if i>target_number: break\n",
    "        try:\n",
    "            #Perturbation\n",
    "            perturbed_spectrum = spectrum + np.random.normal(0, spectrum_unc) \n",
    "            perturbed_wavelengths = wavelengths + np.random.normal(0, wavelength_unc)\n",
    "            #Extract Parameters\n",
    "            popt, pcov = curve_fit(double_gaussian, perturbed_wavelengths, perturbed_spectrum, \n",
    "                                   p0=guess, sigma=spectrum_unc, absolute_sigma=True, maxfev=10000)\n",
    "            #Residual Statistics\n",
    "            statistics = residual_statistics(wavelengths, spectrum, spectrum_unc, popt)\n",
    "            reduced_chi_squared = statistics[2]\n",
    "            #if (reduced_chi_squared > 1.7) | (reduced_chi_squared < 0.8): continue\n",
    "            #Find doppler shift\n",
    "            doppler_shift = popt[4] - popt[1]\n",
    "            #if doppler_shift > .45: continue\n",
    "            wavelength_shifts.append(doppler_shift)\n",
    "            popt_array[i, :] = popt\n",
    "        except Exception as e:\n",
    "            print(f\"Error during fit: {e}\")\n",
    "            continue  # Skip this iteration if the fit fails\n",
    "    print(f\"Iterations Ran: {len(wavelength_shifts)}\")\n",
    "\n",
    "    # Calculate Wavelength Centers\n",
    "    popt_means = np.mean(popt_array, axis=0)\n",
    "    mu1_mean = popt_means[1]\n",
    "    mu2_mean = popt_means[4]\n",
    "    # Calculate Mean and Std of Wavelength (Doppler) Shifts \n",
    "    wavelength_shifts = np.array(wavelength_shifts)\n",
    "    mean_shift = np.mean(wavelength_shifts)\n",
    "    std_shift = np.std(wavelength_shifts)\n",
    "    # Calculate Doppler Shifted velocity and uncertainty\n",
    "    mean_doppler_shift_velocity = doppler_shift_function(mu2=mean_shift, mu1=0, lambda_rest=lambda_rest)\n",
    "    mean_doppler_shift_uncertainty = doppler_shift_function(mu2=std_shift, mu1=0, lambda_rest=lambda_rest)\n",
    "    #Doppler Stats\n",
    "    doppler_stats = [mean_shift, std_shift, mean_doppler_shift_velocity, mean_doppler_shift_uncertainty]\n",
    "\n",
    "    #Add to Dict\n",
    "    monte_carlo_stats[guess_key] = (doppler_stats, wavelength_shifts, popt_means)\n",
    "    print(f\"{labels[guess_key]} Monte Carlo Simulation Results\")\n",
    "    print(f\"Mean Wavelength Shift: {mean_shift:.3f} nm\")\n",
    "    print(f\"Uncertainty in Wavelength Shift: $\\\\pm$ {std_shift:.3f} nm\")\n",
    "    print(f\"Mean Doppler Shift: {mean_doppler_shift_velocity:.3f} km/s\")\n",
    "    print(f\"Uncertainty in Doppler Shift: $\\\\pm$ {mean_doppler_shift_uncertainty:.3f} km/s\\n\")\n",
    "\n",
    "\n",
    "# Plot distribution of Doppler shifts\n",
    "fig, axes = plt.subplots(2,2, figsize=(10,10))\n",
    "axes = axes.flatten()\n",
    "for ax, (key, value) in zip(axes, monte_carlo_stats.items()):\n",
    "    doppler_stats = value[0]\n",
    "    wavelength_shifts = value[1]\n",
    "    mean_shift = doppler_stats[0]\n",
    "    std_shift = doppler_stats[1]\n",
    "    bins = np.arange(min(wavelength_shifts), max(wavelength_shifts), 0.02)  # Adjust bin width if necessary\n",
    "    ax.hist(wavelength_shifts, bins=bins, alpha=0.75, color='blue', label='Doppler Shifts')\n",
    "    ax.axvline(mean_shift, color='red', linestyle='--', label=f\"$\\\\mu$: {mean_shift:.3f} $\\\\pm$ {std_shift:.3f} nm\")\n",
    "    ax.axvline(mean_shift + std_shift, color='red', linestyle='--', alpha=.3, )#label=f'$\\\\mu + \\\\sigma$: {mean_shift + std_shift:.3f}', )\n",
    "    ax.axvline(mean_shift - std_shift, color='red', linestyle='--', alpha=.3, )#label=f'$\\\\mu - \\\\sigma$: {mean_shift - std_shift:.3f}', )\n",
    "    ax.plot([], [], label=f\"$\\\\bar{{v}}$ = {doppler_stats[2]:.3f} \\n$\\\\pm$ {doppler_stats[3]:.3f} km/s\", color='none')    \n",
    "    ax.set_xlabel('Wavelength Shift (nm)')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    label = labels[key].replace(\" , \\\\gamma\", \"\")\n",
    "    ax.set_title(f'Monte Carlo Simulation of Doppler Shift for {label}')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "plt.subplots_adjust(left=0.1, bottom=0.1, right=0.95, top=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560632ed-caff-42ce-8668-c23b5bf1e922",
   "metadata": {},
   "source": [
    "### Radial Velocity Curve (Theoretical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c805ee3c-0ed6-4b4a-b41d-ba28eef87f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Period = 3.96004753 \n",
    "Period_hrs = Period*24\n",
    "def radial_velocity_curve(t):\n",
    "    w = (np.pi*2) / Period_hrs\n",
    "    A = 219\n",
    "    return A*np.cos(w*t)\n",
    "times = np.linspace(-Period_hrs,Period_hrs,10000)\n",
    "plt.plot(times, radial_velocity_curve(times))\n",
    "plt.axhline(y=0,c='black',lw=1)\n",
    "plt.axvline(x=0,c='black',lw=1)\n",
    "\n",
    "def wavelength_shift(wavelength, velocity):\n",
    "    '''\n",
    "    wavelength in nm\n",
    "    velocity in km/s\n",
    "    returns wavelength shift in angstroms\n",
    "    '''\n",
    "    wavelengths = {'alpha':656.3,\n",
    "                   'beta':486.1,\n",
    "                   'gamma':434.0}\n",
    "    wavelength = wavelengths[wavelength]\n",
    "    c = 3.0e8\n",
    "    return ((1e-6*wavelength*velocity)/c) * 1e10\n",
    "\n",
    "#delta1 = #-Period_hrs/2 + 61.5\n",
    "#delta2 = #-Period_hrs/2 + 64\n",
    "\n",
    "'''These are the times when the radial velocities \n",
    "reach the limit of what we can see on the spectrograph:'''\n",
    "tmx1 = 16.5300020187\n",
    "tmx2 = 30.9905683413\n",
    "tmx3 = 64.0505723787\n",
    "tmx4 = 78.5111387013\n",
    "plt.scatter(x=tmx1, y=radial_velocity_curve(tmx1),c='red',s=20)\n",
    "plt.scatter(x=tmx2, y=radial_velocity_curve(tmx2),c='red',s=20)\n",
    "plt.scatter(x=tmx3, y=radial_velocity_curve(tmx3),c='red',s=20)\n",
    "plt.scatter(x=tmx4, y=radial_velocity_curve(tmx4),c='red',s=20)\n",
    "\n",
    "'''Here is for visualizing a theoretical start and end time.\n",
    "Choose t1 and t2. The area under the graph between these two points \n",
    "will be filled in.\n",
    "'''\n",
    "#t1 = Period_hrs/4 + 4 + (16.0/60.0)\n",
    "#t2 = Period_hrs/4 + 4 + (26.0/60.0)\n",
    "t1 = Period_hrs/4 + 4\n",
    "t2 = Period_hrs/4 + 7\n",
    "#t1 = 4\n",
    "#t2 = 8\n",
    "\n",
    "# tvalues=np.linspace(t1,t2,100)\n",
    "# plt.fill_between(tvalues,radial_velocity_curve(tvalues))\n",
    "# plt.plot(tvalues,radial_velocity_curve(tvalues),color='black')\n",
    "# plt.scatter(x=t1, y=radial_velocity_curve(t1),c='black',s=20)\n",
    "# plt.text(t1+.01,radial_velocity_curve(t1)+7,f'{radial_velocity_curve(t1):.2f}',size=10, c='r')\n",
    "# plt.scatter(x=t2, y=radial_velocity_curve(t2),c='black',s=20)\n",
    "# plt.text(t2+.01,radial_velocity_curve(t2)+7,f'{radial_velocity_curve(t2):.2f}',size=10, c='r')\n",
    "print(f'Radial Velocity Difference Between t1 and t2: \\\n",
    "{(radial_velocity_curve(t1) - radial_velocity_curve(t2)):.3f} km/s \\nTime Duration: {(t2-t1):.3f} hrs')\n",
    "print(f'v(t=t1)={abs(radial_velocity_curve(t1)):.3f} km/s \\nv(t=t2)={abs(radial_velocity_curve(t2)):.3f} km/s')\n",
    "print(f\"Pixel shift for v1: {wavelength_shift('alpha',abs(radial_velocity_curve(t1))):.3f} angstroms(pixels)\")\n",
    "print(f\"Pixel shift for v2: {wavelength_shift('alpha',abs(radial_velocity_curve(t2))):.3f} angstroms(pixels)\")\n",
    "\n",
    "plt.xlabel('Time (hrs)')\n",
    "plt.ylabel('Radial Velocity (km/s)')\n",
    "plt.title('Radial Velocity vs Time')\n",
    "plt.plot()\n",
    "\n",
    "#peak at 19:34\n",
    "time_obs_ha_set1 = 4 + (26/60) + 52/60#ha set 1 (00:50)\n",
    "time_obs_ha_set2 = 4 + (26/60) +  3.5#ha set 2 \n",
    "time_obs_hbg_set1 = 4 + (26/60) + 2 + (20/60)#ha set 1 \n",
    "time_obs_hbg_set2 = 4 + (26/60) +  2 + (36/60)  #ha set 2 \n",
    "print(f\"Expected velocity for Ha set 1 ({time_obs_ha_set1:.3f} hrs): {radial_velocity_curve(time_obs_ha_set1):.3f}\")\n",
    "print(f\"Expected velocity for Ha set2 ({time_obs_ha_set2:.3f} hrs): {radial_velocity_curve(time_obs_ha_set2):.3f}\")\n",
    "print(f\"Expected velocity for Hbg set 1 ({time_obs_hbg_set1:.3f} hrs): {radial_velocity_curve(time_obs_hbg_set1):.3f}\")\n",
    "print(f\"Expected velocity for Hbg set2 ({time_obs_hbg_set2:.3f} hrs): {radial_velocity_curve(time_obs_hbg_set2):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f6266-7f75-432e-a439-7eb44fd4417b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56252000-c830-4076-a2f4-ac551a6fccd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09fd52a-4df8-4529-9287-3a1b10843f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f168c-4372-4056-9c23-310ddab44712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072bf9a-d01c-4a98-b071-4d38cd1047b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b97793-22c5-4634-9c78-11fa63797350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4e76a-3140-43fd-8bb1-8891f9d583e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8238b0-d767-4a28-a8a8-aa448bd77f93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
